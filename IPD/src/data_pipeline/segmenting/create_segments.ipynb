{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854aa776",
   "metadata": {},
   "outputs": [],
   "source": [
    "PODCAST_NAME = 'gradient_perspectives'\n",
    "\n",
    "# episode index\n",
    "START_INDEX = 0\n",
    "END_INDEX = 146\n",
    "\n",
    "# only consider characters in podcast episode up to this limit\n",
    "CHARACTER_LIMIT = 100000 \n",
    "\n",
    "MIN_CHARACTERS = 300\n",
    "MAX_CHARACTERS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bafef57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.config import PODCAST_TANSCRIBED_PATHS, PODCAST_SEGMENTED_DIRS\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fac1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TOPIC_LIST = \"\"\"\n",
    "You are a state-of-the-art podcast transcript analyzer.\n",
    "Your task is to divide the ENTIRE transcript into 5-40 consecutive NON-OVERLAPPING segments based on topics.\n",
    "The segments should contain 10-20 sentences and represent self-contained topics.\n",
    "Commercial breaks should be labeled as a seperate segment.\n",
    "Your only output should be a chronological list of topic labels (3-5 words each).\n",
    "\n",
    "For each of the topics provide the following:\n",
    "[\n",
    "{\n",
    "\"topic_description\": \"[topic_label]\",\n",
    "},\n",
    "{\n",
    "\"topic_description\": \"[topic_label]\",\n",
    "}\n",
    "]\n",
    "\n",
    "PODCAST TRANSCRIPT: '''\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_ADD_FIRST_LAST_5_WORDS = f\"\"\"\n",
    "You are adding two fields to the previous step in the process. \n",
    "\n",
    "The previous prompt was: *** {PROMPT_TOPIC_LIST} ***\n",
    "\n",
    "YOUR GOAL: For each of the topics provide the following:\n",
    "[\n",
    "{{\n",
    "\"topic_description\": \"[topic_label]\",\n",
    "\"first_5_words_topic\": \"[exact 5 words from transcript]\",\n",
    "\"last_5_words_topic\": \"[exact 5 words from transcript]\",\n",
    "}},\n",
    "{{\n",
    "\"topic_description\": \"[topic_label]\",\n",
    "\"first_5_words_topic\": \"[exact 5 words from transcript]\",\n",
    "\"last_5_words_topic\": \"[exact 5 words from transcript]\",\n",
    "}}\n",
    "]\n",
    "Note: \n",
    "- the first 5 words should be the beginning of a sentence and the last 5 words the ending of a sentence.\n",
    "- do not change given topic labels.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentTopicsLLM:\n",
    "   \"\"\"\n",
    "   Creates formatted segmented topics and add boundaries using an episode text as input.\n",
    "   Format is (JSON):\n",
    "   [{\n",
    "        \"topic_description\": \"[topic_label]\",\n",
    "        \"first_5_words_topic\": \"[exact 5 words from transcript]\",\n",
    "        \"last_5_words_topic\": \"[exact 5 words from transcript]\",\n",
    "    }]\n",
    "   \"\"\"\n",
    "   def __init__(self, api_key=None):\n",
    "       self.api_key = api_key or os.getenv(\"GOOGLE_API_KEY\")\n",
    "       self.model = \"gemini-2.5-flash\"\n",
    "       \n",
    "   def _call_api(self, prompt):\n",
    "       return self.client.models.generate_content(\n",
    "           model=self.model,\n",
    "           contents=prompt,\n",
    "           config={\n",
    "               \"temperature\": 0.0, \n",
    "               \"response_mime_type\": \"application/json\",\n",
    "               \"thinking_config\": {\n",
    "                   \"thinking_budget\": 0\n",
    "               }\n",
    "            }\n",
    "       )\n",
    "   \n",
    "   def segment_episode(self, episode_text, print_diagnostics):\n",
    "       \n",
    "       topic_segments = self._extract_topics(episode_text)\n",
    "       if print_diagnostics:\n",
    "        self._print_results(topic_segments, \"creating topic segments\")\n",
    "       \n",
    "       topics_with_boundaries = self._add_boundaries(topic_segments.text, episode_text)\n",
    "       if print_diagnostics:\n",
    "           self._print_results(topics_with_boundaries, \"addding first and last 5 words to segments\")\n",
    "       \n",
    "       return topics_with_boundaries\n",
    "\n",
    "   def _extract_topics(self, episode_text):\n",
    "       prompt = f\"{PROMPT_TOPIC_LIST}{episode_text}'''\"\n",
    "       return self._call_api(prompt)\n",
    "   \n",
    "   def _add_boundaries(self, topic_descriptions, episode_text):\n",
    "       prompt = f\"{PROMPT_ADD_FIRST_LAST_5_WORDS} TOPICS: {topic_descriptions} EPISODE TEXT: {episode_text}\"\n",
    "       return self._call_api(prompt)\n",
    "\n",
    "   def _print_results(self, response, action_name):\n",
    "       print(response)\n",
    "       print(f\"Tokens used for {action_name}: {response.usage_metadata.total_token_count}\")\n",
    "       print(response.text)\n",
    "       print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b0cc1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentProcessor:\n",
    "    @staticmethod\n",
    "    def process_segments(segments_json, episode_text, episode_data, episode_idx, print_diagnostics):\n",
    "        try:\n",
    "            segments = json.loads(segments_json)\n",
    "        except json.JSONDecodeError as e:\n",
    "            if print_diagnostics:\n",
    "                print(f\"Skipping episode {episode_idx} due to JSON decode error\")\n",
    "            return []\n",
    "\n",
    "        processed_segments = []\n",
    "        \n",
    "        for segment_idx, segment in enumerate(segments):\n",
    "            first_5_words = segment.get('first_5_words_topic', '')\n",
    "            last_5_words = segment.get('last_5_words_topic', '')\n",
    "            \n",
    "            if first_5_words:\n",
    "                start_pos = episode_text.find(first_5_words)\n",
    "                if start_pos != -1:\n",
    "                    if last_5_words:\n",
    "                        end_search_start = start_pos + len(first_5_words)\n",
    "                        end_pos = episode_text.find(last_5_words, end_search_start)\n",
    "                        if end_pos != -1:\n",
    "                            end_pos = end_pos + len(last_5_words)\n",
    "                        else:\n",
    "                            if segment_idx < len(segments) - 1:\n",
    "                                next_segment = segments[segment_idx + 1]\n",
    "                                next_first_5_words = next_segment.get('first_5_words_topic', '')\n",
    "                                end_pos = episode_text.find(next_first_5_words, start_pos + len(first_5_words))\n",
    "                                if end_pos == -1:\n",
    "                                    end_pos = len(episode_text)\n",
    "                            else:\n",
    "                                end_pos = len(episode_text)\n",
    "                    else:\n",
    "                        if segment_idx < len(segments) - 1:\n",
    "                            next_segment = segments[segment_idx + 1]\n",
    "                            next_first_5_words = next_segment.get('first_5_words_topic', '')\n",
    "                            end_pos = episode_text.find(next_first_5_words, start_pos + len(first_5_words))\n",
    "                            if end_pos == -1:\n",
    "                                end_pos = len(episode_text)\n",
    "                        else:\n",
    "                            end_pos = len(episode_text)\n",
    "\n",
    "                    ordered_segment = {\n",
    "                        'episode_id': episode_idx,\n",
    "                        'podcast_title': episode_data['podcast_title'],\n",
    "                        'episode_title': episode_data['episode_title'],\n",
    "                        'date': pd.to_datetime(episode_data['date']).strftime('%Y-%m-%d'),\n",
    "                        'segment_id': segment_idx,\n",
    "                        'topic_description_llm': segment.get('topic_description', ''),\n",
    "                        'first_5_words': first_5_words,\n",
    "                        'start_index': start_pos,\n",
    "                        'end_index': end_pos,\n",
    "                        'character_length': end_pos - start_pos,\n",
    "                        'segment_text': episode_text[start_pos:end_pos]\n",
    "                    }\n",
    "\n",
    "                    processed_segments.append(ordered_segment)\n",
    "                else:\n",
    "                    if print_diagnostics:\n",
    "                        print(f\"WARNING: Could not find '{first_5_words}' for segment {segment_idx}: {segment.get('topic_description')}\")\n",
    "                    \n",
    "        return processed_segments\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_segments(segments, output_dir, episode_idx):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(f'{output_dir}/episode_{episode_idx}.json', 'w') as f:\n",
    "            json.dump(segments, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2371f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_episodes_batch(PODCAST_NAME, START_INDEX, END_INDEX, CHARACTER_LIMIT, print_diagnostics=True, trottle_for_free_api=True):\n",
    "   \n",
    "   input_file_path = PODCAST_TANSCRIBED_PATHS[PODCAST_NAME]\n",
    "   output_dir = PODCAST_SEGMENTED_DIRS[PODCAST_NAME]\n",
    "\n",
    "   df = pd.read_parquet(input_file_path)\n",
    "\n",
    "   print(f\"Starting segmentation of {len(df.iloc[START_INDEX:END_INDEX+1])} podcast episodes...\")\n",
    "   print(\"Input data of first episode in selection:\")\n",
    "\n",
    "   if print_diagnostics:\n",
    "       print(df.iloc[START_INDEX])\n",
    "\n",
    "   segmentor = SegmentTopicsLLM()\n",
    "   processor = SegmentProcessor()\n",
    "\n",
    "   for i, (idx, episode) in enumerate(df.iloc[START_INDEX:END_INDEX+1].iterrows()):\n",
    "       \n",
    "       if os.path.exists(f'{output_dir}/episode_{idx}.json'):\n",
    "            print(f\"Skipping already processed episode {idx}\")\n",
    "            continue\n",
    "       \n",
    "       episode_text = episode['transcript'][:CHARACTER_LIMIT]\n",
    "       \n",
    "       print(f\"Processing episode {i + 1} of {len(df.iloc[START_INDEX:END_INDEX+1])}, #{idx} {episode['podcast_title']}, episode: {episode['episode_title']}...\")       \n",
    "       if print_diagnostics:\n",
    "           print(f'Character length of episode: {len(episode_text)}\\n')\n",
    "       \n",
    "       topics_with_boundaries = segmentor.segment_episode(episode_text, print_diagnostics).text\n",
    "\n",
    "       processed_segments = processor.process_segments(topics_with_boundaries, episode_text, episode, idx, print_diagnostics)\n",
    "       \n",
    "       processor.save_segments(processed_segments, output_dir, idx)\n",
    "       \n",
    "       if trottle_for_free_api:\n",
    "        import time\n",
    "        time.sleep(60) # seconds\n",
    "\n",
    "   \n",
    "   print(\"Segmentation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_segments_by_length(PODCAST_NAME, START_INDEX, END_INDEX, MIN_CHARACTERS, MAX_CHARACTERS):\n",
    "    \n",
    "    output_dir = PODCAST_SEGMENTED_DIRS[PODCAST_NAME]\n",
    "    \n",
    "    for episode_idx in range(START_INDEX, END_INDEX + 1):\n",
    "        file_path = f'{output_dir}/episode_{episode_idx}.json'\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                segments = json.load(f)\n",
    "            \n",
    "            filtered_segments = [\n",
    "                segment for segment in segments \n",
    "                if MIN_CHARACTERS <= segment['character_length'] <= MAX_CHARACTERS\n",
    "            ]\n",
    "            \n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(filtered_segments, f, indent=2)\n",
    "            \n",
    "            print(f\"Episode {episode_idx}: Kept {len(filtered_segments)} of {len(segments)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb34414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting segmentation of 147 podcast episodes...\n",
      "Input data of first episode in selection:\n",
      "Processing episode 1 of 147, #0 The_Gradient_Perspectives_on_AI, episode: Lt. General Jack Shanahan: AI in the DoD, Project Maven, and Bridging the Tech-DoD Gap...\n",
      "Processing episode 2 of 147, #1 The_Gradient_Perspectives_on_AI, episode: Laura Weidinger: Ethical Risks, Harms, and Alignment of Large Language Models...\n",
      "Processing episode 3 of 147, #2 The_Gradient_Perspectives_on_AI, episode: Richard Socher: Re-Imagining Search...\n",
      "Processing episode 4 of 147, #3 The_Gradient_Perspectives_on_AI, episode: Joel Simon on AI art and Artbreeder...\n",
      "Processing episode 5 of 147, #4 The_Gradient_Perspectives_on_AI, episode: Russ Maschmeyer: Spatial Commerce and AI in Retail...\n",
      "Processing episode 6 of 147, #5 The_Gradient_Perspectives_on_AI, episode: Seth Lazar: Normative Philosophy of Computing...\n",
      "Processing episode 7 of 147, #6 The_Gradient_Perspectives_on_AI, episode: Vivek Natarajan: Towards Biomedical AI...\n",
      "Processing episode 8 of 147, #7 The_Gradient_Perspectives_on_AI, episode: Scott Aaronson: Against AI Doomerism...\n",
      "Processing episode 9 of 147, #8 The_Gradient_Perspectives_on_AI, episode: Cameron Jones & Sean Trott: Understanding, Grounding, and Reference in LLMs...\n",
      "Processing episode 10 of 147, #9 The_Gradient_Perspectives_on_AI, episode: Alex Tamkin on Self-Supervised Learning and Large Language Models...\n",
      "Processing episode 11 of 147, #10 The_Gradient_Perspectives_on_AI, episode: Connor Leahy on EleutherAI, Replicating GPT-2/GPT-3, AI Risk and Alignment...\n",
      "Processing episode 12 of 147, #11 The_Gradient_Perspectives_on_AI, episode: Michael Levin & Adam Goldstein: Intelligence and its Many Scales...\n",
      "Processing episode 13 of 147, #12 The_Gradient_Perspectives_on_AI, episode: Philip Goff: Panpsychism as a Theory of Consciousness...\n",
      "Processing episode 14 of 147, #13 The_Gradient_Perspectives_on_AI, episode: Rishi Bommasani on Foundation Models...\n",
      "Processing episode 15 of 147, #14 The_Gradient_Perspectives_on_AI, episode: Meredith Ringel Morris: Generative AI's HCI Moment...\n",
      "Processing episode 16 of 147, #15 The_Gradient_Perspectives_on_AI, episode: Antoine Blondeau: Alpha Intelligence Capital and Investing in AI...\n",
      "Processing episode 17 of 147, #16 The_Gradient_Perspectives_on_AI, episode: Jonathan Frankle: From Lottery Tickets to LLMs...\n",
      "Processing episode 18 of 147, #17 The_Gradient_Perspectives_on_AI, episode: Kevin K. Yang: Engineering Proteins with ML...\n",
      "Processing episode 19 of 147, #18 The_Gradient_Perspectives_on_AI, episode: Pete Wolfendale: The Revenge of Reason...\n",
      "Processing episode 20 of 147, #19 The_Gradient_Perspectives_on_AI, episode: Peter Lee: Computing Theory and Practice, and GPT-4's Impact...\n",
      "Processing episode 21 of 147, #20 The_Gradient_Perspectives_on_AI, episode: Irene Solaiman: AI Policy and Social Impact...\n",
      "Processing episode 22 of 147, #21 The_Gradient_Perspectives_on_AI, episode: Tal Linzen: Psycholinguistics and Language Modeling...\n",
      "Processing episode 23 of 147, #22 The_Gradient_Perspectives_on_AI, episode: Venkatesh Rao: Protocols, Intelligence, and Scaling...\n",
      "Processing episode 24 of 147, #23 The_Gradient_Perspectives_on_AI, episode: Jeremy Howard on Kaggle, Enlitic, and fast.ai...\n",
      "Processing episode 25 of 147, #24 The_Gradient_Perspectives_on_AI, episode: Zachary Lipton: Where Machine Learning Falls Short...\n",
      "Processing episode 26 of 147, #25 The_Gradient_Perspectives_on_AI, episode: Joe Edelman: Meaning-Aligned AI...\n",
      "Processing episode 27 of 147, #26 The_Gradient_Perspectives_on_AI, episode: Vera Liao: AI Explainability and Transparency...\n",
      "Processing episode 28 of 147, #27 The_Gradient_Perspectives_on_AI, episode: Ryan Tibshirani: Statistics, Nonparametric Regression, Conformal Prediction...\n",
      "Processing episode 29 of 147, #28 The_Gradient_Perspectives_on_AI, episode: Arjun Ramani & Zhengdong Wang: Why Transformative AI is Really, Really Hard to Achieve...\n",
      "Processing episode 30 of 147, #29 The_Gradient_Perspectives_on_AI, episode: Thomas Dietterich: From the Foundations...\n",
      "Processing episode 31 of 147, #30 The_Gradient_Perspectives_on_AI, episode: Percy Liang on Machine Learning Robustness, Foundation Models, and Reproducibility...\n",
      "Processing episode 32 of 147, #31 The_Gradient_Perspectives_on_AI, episode: Kanjun Qiu and Josh Albrecht: Generally Intelligent...\n",
      "Processing episode 33 of 147, #32 The_Gradient_Perspectives_on_AI, episode: Eric Jang: AI is Good For You...\n",
      "Processing episode 34 of 147, #33 The_Gradient_Perspectives_on_AI, episode: Joon Park: Generative Agents and Human-Computer Interaction...\n",
      "Processing episode 35 of 147, #34 The_Gradient_Perspectives_on_AI, episode: Sara Hooker: Cohere For AI, the Hardware Lottery, and DL Tradeoffs...\n",
      "Processing episode 36 of 147, #35 The_Gradient_Perspectives_on_AI, episode: Hello World from The Gradient Podcast!...\n",
      "Processing episode 37 of 147, #36 The_Gradient_Perspectives_on_AI, episode: Ken Liu: What Science Fiction Can Teach Us...\n",
      "Processing episode 38 of 147, #37 The_Gradient_Perspectives_on_AI, episode: Kathleen Fisher: DARPA and AI for National Security...\n",
      "Processing episode 39 of 147, #38 The_Gradient_Perspectives_on_AI, episode: Evan Hubinger on Effective Altruism and AI Safety...\n",
      "Processing episode 40 of 147, #39 The_Gradient_Perspectives_on_AI, episode: Ed Grefenstette: Language, Semantics, Cohere...\n",
      "Processing episode 41 of 147, #40 The_Gradient_Perspectives_on_AI, episode: Ted Gibson: The Structure and Purpose of Language...\n",
      "Processing episode 42 of 147, #41 The_Gradient_Perspectives_on_AI, episode: Nicholas Thompson: AI and Journalism...\n",
      "Processing episode 43 of 147, #42 The_Gradient_Perspectives_on_AI, episode: Yann LeCun on his Start in Research and Self-Supervised Learning...\n",
      "Processing episode 44 of 147, #43 The_Gradient_Perspectives_on_AI, episode: Hattie Zhou: Lottery Tickets and Algorithmic Reasoning in LLMs...\n",
      "Processing episode 45 of 147, #44 The_Gradient_Perspectives_on_AI, episode: Sergiy Nesterenko: Automating Circuit Board Design...\n",
      "Processing episode 46 of 147, #45 The_Gradient_Perspectives_on_AI, episode: Andrew Feldman: Cerebras and AI Hardware...\n",
      "Processing episode 47 of 147, #46 The_Gradient_Perspectives_on_AI, episode: L.M. Sacasas: The Questions Concerning Technology...\n",
      "Processing episode 48 of 147, #47 The_Gradient_Perspectives_on_AI, episode: Sergey Levine on Robot Learning & Offline RL...\n",
      "Processing episode 49 of 147, #48 The_Gradient_Perspectives_on_AI, episode: Sebastian Raschka: AI Education and Research...\n",
      "Processing episode 50 of 147, #49 The_Gradient_Perspectives_on_AI, episode: Sasha Luccioni: Connecting the Dots Between AI's Environmental and Social Impacts...\n",
      "Processing episode 51 of 147, #50 The_Gradient_Perspectives_on_AI, episode: Dan Hart and Michelle Michael: Bringing AI to Students in New South Wales...\n",
      "Processing episode 52 of 147, #51 The_Gradient_Perspectives_on_AI, episode: Eric Jang on Robots Learning at Google and Generalization via Language...\n",
      "Processing episode 53 of 147, #52 The_Gradient_Perspectives_on_AI, episode: Sewon Min: The Science of Natural Language...\n",
      "Processing episode 54 of 147, #53 The_Gradient_Perspectives_on_AI, episode: Jeff Clune: Genetic Algorithms, Quality-Diversity, Curiosity...\n",
      "Processing episode 55 of 147, #54 The_Gradient_Perspectives_on_AI, episode: Abubakar Abid on AI for Genomics, Gradio, and the Fatima Fellowship...\n",
      "Processing episode 56 of 147, #55 The_Gradient_Perspectives_on_AI, episode: 2024 in AI, with Nathan Benaich...\n",
      "Processing episode 57 of 147, #56 The_Gradient_Perspectives_on_AI, episode: Kyunghyun Cho: Neural Machine Translation, Language, and Doing Good Science...\n",
      "Processing episode 58 of 147, #57 The_Gradient_Perspectives_on_AI, episode: Greg Yang on Communicating Research, Tensor Programs, and µTransfer...\n",
      "Processing episode 59 of 147, #58 The_Gradient_Perspectives_on_AI, episode: Nathan Benaich: The State of AI Report...\n",
      "Processing episode 60 of 147, #59 The_Gradient_Perspectives_on_AI, episode: Suhail Doshi: The Future of Computer Vision...\n",
      "Processing episode 61 of 147, #60 The_Gradient_Perspectives_on_AI, episode: Peter Tse: The Neuroscience of Consciousness and Free Will...\n",
      "Processing episode 62 of 147, #61 The_Gradient_Perspectives_on_AI, episode: Chip Huyen: Machine Learning Tools and Systems...\n",
      "Processing episode 63 of 147, #62 The_Gradient_Perspectives_on_AI, episode: Joel Lehman: Open-Endedness and Evolution through Large Models...\n",
      "Processing episode 64 of 147, #63 The_Gradient_Perspectives_on_AI, episode: Sasha Rush: Building Better NLP Systems...\n",
      "Processing episode 65 of 147, #64 The_Gradient_Perspectives_on_AI, episode: Shiv Rao: Enabling Better Patient Care with AI...\n",
      "Processing episode 66 of 147, #65 The_Gradient_Perspectives_on_AI, episode: Linus Lee: At the Boundary of Machine and Mind...\n",
      "Processing episode 67 of 147, #66 The_Gradient_Perspectives_on_AI, episode: Max Braun: Teaching Robots to Help People in their Everyday Lives...\n",
      "Processing episode 68 of 147, #67 The_Gradient_Perspectives_on_AI, episode: Blair Attard-Frost: Canada’s AI strategy and the ethics of AI business practices...\n",
      "Processing episode 69 of 147, #68 The_Gradient_Perspectives_on_AI, episode: Suresh Venkatasubramanian: An AI Bill of Rights...\n",
      "Processing episode 70 of 147, #69 The_Gradient_Perspectives_on_AI, episode: Judy Fan: Reverse Engineering the Human Cognitive Toolkit...\n",
      "Processing episode 71 of 147, #70 The_Gradient_Perspectives_on_AI, episode: Nao Tokui: \"Surfing\" Musical Creativity with AI...\n",
      "Processing episode 72 of 147, #71 The_Gradient_Perspectives_on_AI, episode: Chelsea Finn on Meta Learning & Model Based Reinforcement Learning...\n",
      "Processing episode 73 of 147, #72 The_Gradient_Perspectives_on_AI, episode: David Chalmers on AI and Consciousness...\n",
      "Processing episode 74 of 147, #73 The_Gradient_Perspectives_on_AI, episode: Terry Winograd: AI, HCI, Language, and Cognition...\n",
      "Processing episode 75 of 147, #74 The_Gradient_Perspectives_on_AI, episode: Melanie Mitchell: Abstraction and Analogy in AI...\n",
      "Processing episode 76 of 147, #75 The_Gradient_Perspectives_on_AI, episode: Rosanne Liu: Paths in AI Research and ML Collective...\n",
      "Processing episode 77 of 147, #76 The_Gradient_Perspectives_on_AI, episode: Alexander Veysov on Self-Teaching AI and Creating Open Speech-To-Text...\n",
      "Processing episode 78 of 147, #77 The_Gradient_Perspectives_on_AI, episode: Yoshua Bengio: The Past, Present, and Future of Deep Learning...\n",
      "Processing episode 79 of 147, #78 The_Gradient_Perspectives_on_AI, episode: Azeem Azhar: The Exponential View...\n",
      "Processing episode 80 of 147, #79 The_Gradient_Perspectives_on_AI, episode: François Chollet: Keras and Measures of Intelligence...\n",
      "Processing episode 81 of 147, #80 The_Gradient_Perspectives_on_AI, episode: Clive Thompson: Tales of Technology...\n",
      "Processing episode 82 of 147, #81 The_Gradient_Perspectives_on_AI, episode: Devi Parikh on Generative Art & AI for Creativity...\n",
      "Processing episode 83 of 147, #82 The_Gradient_Perspectives_on_AI, episode: Anna Rogers on the Flaws of Peer Review in AI...\n",
      "Processing episode 84 of 147, #83 The_Gradient_Perspectives_on_AI, episode: Brigham Hyde: AI for Clinical Decision-Making...\n",
      "Processing episode 85 of 147, #84 The_Gradient_Perspectives_on_AI, episode: Subbarao Kambhampati: Planning, Reasoning, and Interpretability in the Age of LLMs...\n",
      "Processing episode 86 of 147, #85 The_Gradient_Perspectives_on_AI, episode: Miles Brundage on AI Misuse and Trustworthy AI...\n",
      "Processing episode 87 of 147, #86 The_Gradient_Perspectives_on_AI, episode: Stuart Russell: The Foundations of Artificial Intelligence...\n",
      "Processing episode 88 of 147, #87 The_Gradient_Perspectives_on_AI, episode: Laurence Liew: AI Singapore...\n",
      "Processing episode 89 of 147, #88 The_Gradient_Perspectives_on_AI, episode: Jeremie Harris: Realistic Alignment and AI Policy...\n",
      "Processing episode 90 of 147, #89 The_Gradient_Perspectives_on_AI, episode: Ben Wellington: ML for Finance and Storytelling through Data...\n",
      "Processing episode 91 of 147, #90 The_Gradient_Perspectives_on_AI, episode: Miles Grimshaw: Benchmark, LangChain, and Investing in AI...\n",
      "Processing episode 92 of 147, #91 The_Gradient_Perspectives_on_AI, episode: 2023 in AI, with Nathan Benaich...\n",
      "Processing episode 93 of 147, #92 The_Gradient_Perspectives_on_AI, episode: Davidad Dalrymple: Towards Provably Safe AI...\n",
      "Processing episode 94 of 147, #93 The_Gradient_Perspectives_on_AI, episode: Catherine Olsson and Nelson Elhage: Anthropic, Understanding Transformers...\n",
      "Processing episode 95 of 147, #94 The_Gradient_Perspectives_on_AI, episode: Raphaël Millière: The Vector Grounding Problem and Self-Consciousness...\n",
      "Processing episode 96 of 147, #95 The_Gradient_Perspectives_on_AI, episode: Luis Voloch: AI and Biology...\n",
      "Processing episode 97 of 147, #96 The_Gradient_Perspectives_on_AI, episode: C. Thi Nguyen: Values, Legibility, and Gamification...\n",
      "Processing episode 98 of 147, #97 The_Gradient_Perspectives_on_AI, episode: Hugo Larochelle: Deep Learning as Science...\n",
      "Processing episode 99 of 147, #98 The_Gradient_Perspectives_on_AI, episode: Manuel & Lenore Blum: The Conscious Turing Machine...\n",
      "Processing episode 100 of 147, #99 The_Gradient_Perspectives_on_AI, episode: Ted Underwood: Machine Learning and the Literary Imagination...\n",
      "Processing episode 101 of 147, #100 The_Gradient_Perspectives_on_AI, episode: Riley Goodside: The Art and Craft of Prompt Engineering...\n",
      "Processing episode 102 of 147, #101 The_Gradient_Perspectives_on_AI, episode: Martin Wattenberg: ML Visualization and Interpretability...\n",
      "Processing episode 103 of 147, #102 The_Gradient_Perspectives_on_AI, episode: Christoffer Holmgård: AI for Video Games...\n",
      "Processing episode 104 of 147, #103 The_Gradient_Perspectives_on_AI, episode: Michael Sipser: Problems in the Theory of Computation...\n",
      "Processing episode 105 of 147, #104 The_Gradient_Perspectives_on_AI, episode: Steve Miller: Will AI Take Your Job? It's Not So Simple....\n",
      "Processing episode 106 of 147, #105 The_Gradient_Perspectives_on_AI, episode: Nick Walton on AI Dungeon and the Future of AI in Games...\n",
      "Processing episode 107 of 147, #106 The_Gradient_Perspectives_on_AI, episode: Lukas Biewald: Crowdsourcing at CrowdFlower and ML Tooling at Weights & Biases...\n",
      "Processing episode 108 of 147, #107 The_Gradient_Perspectives_on_AI, episode: Ryan Drapeau: Battling Fraud with ML at Stripe...\n",
      "Processing episode 109 of 147, #108 The_Gradient_Perspectives_on_AI, episode: Soumith Chintala: PyTorch...\n",
      "Processing episode 110 of 147, #109 The_Gradient_Perspectives_on_AI, episode: Yannic Kilcher on Being an AI Researcher and Educator...\n",
      "Processing episode 111 of 147, #110 The_Gradient_Perspectives_on_AI, episode: Evan Ratliff: Our Future with Voice Agents...\n",
      "Processing episode 112 of 147, #111 The_Gradient_Perspectives_on_AI, episode: Been Kim: Interpretable Machine Learning...\n",
      "Processing episode 113 of 147, #112 The_Gradient_Perspectives_on_AI, episode: Peli Grietzer: A Mathematized Philosophy of Literature...\n",
      "Processing episode 114 of 147, #113 The_Gradient_Perspectives_on_AI, episode: David Pfau: Manifold Factorization and AI for Science...\n",
      "Processing episode 115 of 147, #114 The_Gradient_Perspectives_on_AI, episode: Jacob Andreas: Language, Grounding, and World Models...\n",
      "Processing episode 116 of 147, #115 The_Gradient_Perspectives_on_AI, episode: David Thorstad: Bounded Rationality and the Case Against Longtermism...\n",
      "Processing episode 117 of 147, #116 The_Gradient_Perspectives_on_AI, episode: Kristin Lauter: Private AI, Homomorphic Encryption, and AI for Cryptography...\n",
      "Processing episode 118 of 147, #117 The_Gradient_Perspectives_on_AI, episode: Joss Fong: Videomaking, AI, and Science Communication...\n",
      "Processing episode 119 of 147, #118 The_Gradient_Perspectives_on_AI, episode: Harvey Lederman: Propositional Attitudes and Reference in Language Models...\n",
      "Processing episode 120 of 147, #119 The_Gradient_Perspectives_on_AI, episode: Jeffrey Ding on China's AI Dream, the AI 'Arms Race', and AI as a General Purpose Technology...\n",
      "Processing episode 121 of 147, #120 The_Gradient_Perspectives_on_AI, episode: Ben Green: \"Tech for Social Good\" Needs to Do More...\n",
      "Processing episode 122 of 147, #121 The_Gradient_Perspectives_on_AI, episode: Andrew Lee: How AI will Shape the Future of Email...\n",
      "Processing episode 123 of 147, #122 The_Gradient_Perspectives_on_AI, episode: Benjamin Breen: The Intersecting Histories of Psychedelics and AI Research...\n",
      "Processing episode 124 of 147, #123 The_Gradient_Perspectives_on_AI, episode: Shreya Shankar: Machine Learning in the Real World...\n",
      "Processing episode 125 of 147, #124 The_Gradient_Perspectives_on_AI, episode: Talia Ringer: Formal Verification and Deep Learning...\n",
      "Processing episode 126 of 147, #125 The_Gradient_Perspectives_on_AI, episode: Drago Anguelov: Waymo and Autonomous Vehicles...\n",
      "Processing episode 127 of 147, #126 The_Gradient_Perspectives_on_AI, episode: Matt Sheehan: China's AI Strategy and Governance...\n",
      "Processing episode 128 of 147, #127 The_Gradient_Perspectives_on_AI, episode: Christopher Manning: Linguistics and the Development of NLP...\n",
      "Processing episode 129 of 147, #128 The_Gradient_Perspectives_on_AI, episode: Divyansh Kaushik: The Realities of AI Policy...\n",
      "Processing episode 130 of 147, #129 The_Gradient_Perspectives_on_AI, episode: Helena Sarin on being an AI Artist...\n",
      "Processing episode 131 of 147, #130 The_Gradient_Perspectives_on_AI, episode: Varun Ganapathi: AKASA, AI and Healthcare...\n",
      "Processing episode 132 of 147, #131 The_Gradient_Perspectives_on_AI, episode: Yejin Choi: Teaching Machines Common Sense and Morality...\n",
      "Processing episode 133 of 147, #132 The_Gradient_Perspectives_on_AI, episode: Pete Florence: Dense Visual Representations, NeRFs, and LLMs for Robotics...\n",
      "Processing episode 134 of 147, #133 The_Gradient_Perspectives_on_AI, episode: Marc Bellemare: Distributional Reinforcement Learning...\n",
      "Processing episode 135 of 147, #134 The_Gradient_Perspectives_on_AI, episode: Peter Henderson on RL Benchmarking, Climate Impacts of AI, and AI for Law...\n",
      "Processing episode 136 of 147, #135 The_Gradient_Perspectives_on_AI, episode: Anant Agarwal: AI for Education...\n",
      "Processing episode 137 of 147, #136 The_Gradient_Perspectives_on_AI, episode: Kate Park: Data Engines for Vision and Language...\n",
      "Processing episode 138 of 147, #137 The_Gradient_Perspectives_on_AI, episode: Gil Strang: Linear Algebra and Deep Learning...\n",
      "Processing episode 139 of 147, #138 The_Gradient_Perspectives_on_AI, episode: Kevin Dorst: Against Irrationalist Narratives...\n",
      "Processing episode 140 of 147, #139 The_Gradient_Perspectives_on_AI, episode: Some Changes at The Gradient...\n",
      "Processing episode 141 of 147, #140 The_Gradient_Perspectives_on_AI, episode: Stevan Harnad: AI's Symbol Grounding Problem...\n",
      "Processing episode 142 of 147, #141 The_Gradient_Perspectives_on_AI, episode: Joanna Bryson: The Problems of Cognition...\n",
      "Processing episode 143 of 147, #142 The_Gradient_Perspectives_on_AI, episode: Daniel Situnayake: AI on the Edge...\n",
      "Processing episode 144 of 147, #143 The_Gradient_Perspectives_on_AI, episode: Preetum Nakkiran: An Empirical Theory of Deep Learning...\n",
      "Processing episode 145 of 147, #144 The_Gradient_Perspectives_on_AI, episode: Thomas Mullaney: A Global History of the Information Age...\n",
      "Processing episode 146 of 147, #145 The_Gradient_Perspectives_on_AI, episode: Max Woolf: Data Science at BuzzFeed and AI Content Generation...\n",
      "Processing episode 147 of 147, #146 The_Gradient_Perspectives_on_AI, episode: Upol Ehsan on Human-Centered Explainable AI and Social Transparency...\n",
      "Segmentation completed.\n"
     ]
    }
   ],
   "source": [
    "segment_episodes_batch(\n",
    "    PODCAST_NAME, \n",
    "    START_INDEX, \n",
    "    END_INDEX,\n",
    "    CHARACTER_LIMIT, \n",
    "    print_diagnostics=False,\n",
    "    trottle_for_free_api=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be1242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Kept 12 of 12 segments\n",
      "Episode 1: Kept 17 of 17 segments\n",
      "Episode 2: Kept 9 of 13 segments\n",
      "Episode 3: Kept 13 of 14 segments\n",
      "Episode 4: Kept 18 of 18 segments\n",
      "Episode 5: Kept 17 of 19 segments\n",
      "Episode 6: Kept 23 of 23 segments\n",
      "Episode 7: Kept 14 of 17 segments\n",
      "Episode 8: Kept 8 of 11 segments\n",
      "Episode 9: Kept 10 of 11 segments\n",
      "Episode 10: Kept 12 of 15 segments\n",
      "Episode 11: Kept 12 of 13 segments\n",
      "Episode 12: Kept 12 of 14 segments\n",
      "Episode 13: Kept 14 of 17 segments\n",
      "Episode 14: Kept 42 of 42 segments\n",
      "Episode 15: Kept 19 of 20 segments\n",
      "Episode 16: Kept 26 of 28 segments\n",
      "Episode 17: Kept 8 of 9 segments\n",
      "Episode 18: Kept 17 of 21 segments\n",
      "Episode 19: Kept 20 of 20 segments\n",
      "Episode 20: Kept 28 of 28 segments\n",
      "Episode 21: Kept 21 of 24 segments\n",
      "Episode 22: Kept 19 of 24 segments\n",
      "Episode 23: Kept 13 of 14 segments\n",
      "Episode 24: Kept 15 of 17 segments\n",
      "Episode 25: Kept 12 of 13 segments\n",
      "Episode 26: Kept 21 of 23 segments\n",
      "Episode 27: Kept 12 of 14 segments\n",
      "Episode 28: Kept 22 of 25 segments\n",
      "Episode 29: Kept 17 of 21 segments\n",
      "Episode 30: Kept 16 of 17 segments\n",
      "Episode 31: Kept 10 of 13 segments\n",
      "Episode 32: Kept 23 of 24 segments\n",
      "Episode 33: Kept 18 of 19 segments\n",
      "Episode 34: Kept 13 of 14 segments\n",
      "Episode 35: Kept 10 of 10 segments\n",
      "Episode 36: Kept 8 of 13 segments\n",
      "Episode 37: Kept 23 of 23 segments\n",
      "Episode 38: Kept 12 of 16 segments\n",
      "Episode 39: Kept 18 of 19 segments\n",
      "Episode 40: Kept 10 of 17 segments\n",
      "Episode 41: Kept 11 of 13 segments\n",
      "Episode 42: Kept 20 of 20 segments\n",
      "Episode 43: Kept 17 of 18 segments\n",
      "Episode 44: Kept 20 of 21 segments\n",
      "Episode 45: Kept 14 of 14 segments\n",
      "Episode 46: Kept 12 of 16 segments\n",
      "Episode 47: Kept 16 of 17 segments\n",
      "Episode 48: Kept 16 of 17 segments\n",
      "Episode 49: Kept 19 of 21 segments\n",
      "Episode 50: Kept 27 of 29 segments\n",
      "Episode 51: Kept 26 of 27 segments\n",
      "Episode 52: Kept 37 of 38 segments\n",
      "Episode 53: Kept 16 of 17 segments\n",
      "Episode 54: Kept 9 of 9 segments\n",
      "Episode 55: Kept 25 of 26 segments\n",
      "Episode 56: Kept 24 of 25 segments\n",
      "Episode 57: Kept 19 of 19 segments\n",
      "Episode 58: Kept 21 of 22 segments\n",
      "Episode 59: Kept 20 of 20 segments\n",
      "Episode 60: Kept 14 of 18 segments\n",
      "Episode 61: Kept 13 of 13 segments\n",
      "Episode 62: Kept 18 of 22 segments\n",
      "Episode 63: Kept 18 of 19 segments\n",
      "Episode 64: Kept 15 of 15 segments\n",
      "Episode 65: Kept 22 of 24 segments\n",
      "Episode 66: Kept 18 of 19 segments\n",
      "Episode 67: Kept 13 of 14 segments\n",
      "Episode 68: Kept 19 of 20 segments\n",
      "Episode 69: Kept 13 of 17 segments\n",
      "Episode 70: Kept 16 of 16 segments\n",
      "Episode 71: Kept 20 of 20 segments\n",
      "Episode 72: Kept 14 of 14 segments\n",
      "Episode 73: Kept 20 of 27 segments\n",
      "Episode 74: Kept 17 of 17 segments\n",
      "Episode 75: Kept 12 of 13 segments\n",
      "Episode 76: Kept 11 of 11 segments\n",
      "Episode 77: Kept 21 of 23 segments\n",
      "Episode 78: Kept 28 of 31 segments\n",
      "Episode 79: Kept 17 of 18 segments\n",
      "Episode 80: Kept 14 of 17 segments\n",
      "Episode 81: Kept 16 of 18 segments\n",
      "Episode 82: Kept 16 of 17 segments\n",
      "Episode 83: Kept 14 of 14 segments\n",
      "Episode 84: Kept 19 of 23 segments\n",
      "Episode 85: Kept 11 of 12 segments\n",
      "Episode 86: Kept 7 of 10 segments\n",
      "Episode 87: Kept 11 of 11 segments\n",
      "Episode 88: Kept 19 of 20 segments\n",
      "Episode 89: Kept 15 of 15 segments\n",
      "Episode 90: Kept 9 of 11 segments\n",
      "Episode 91: Kept 32 of 33 segments\n",
      "Episode 92: Kept 19 of 22 segments\n",
      "Episode 93: Kept 13 of 13 segments\n",
      "Episode 94: Kept 6 of 11 segments\n",
      "Episode 95: Kept 18 of 18 segments\n",
      "Episode 96: Kept 20 of 23 segments\n",
      "Episode 97: Kept 21 of 24 segments\n",
      "Episode 98: Kept 14 of 18 segments\n",
      "Episode 99: Kept 25 of 25 segments\n",
      "Episode 100: Kept 15 of 16 segments\n",
      "Episode 101: Kept 33 of 37 segments\n",
      "Episode 102: Kept 14 of 16 segments\n",
      "Episode 103: Kept 12 of 13 segments\n",
      "Episode 104: Kept 7 of 9 segments\n",
      "Episode 105: Kept 11 of 14 segments\n",
      "Episode 106: Kept 17 of 18 segments\n",
      "Episode 107: Kept 10 of 13 segments\n",
      "Episode 108: Kept 15 of 16 segments\n",
      "Episode 109: Kept 13 of 14 segments\n",
      "Episode 110: Kept 22 of 24 segments\n",
      "Episode 111: Kept 22 of 23 segments\n",
      "Episode 112: Kept 16 of 19 segments\n",
      "Episode 113: Kept 20 of 22 segments\n",
      "Episode 114: Kept 35 of 37 segments\n",
      "Episode 115: Kept 27 of 27 segments\n",
      "Episode 116: Kept 15 of 17 segments\n",
      "Episode 117: Kept 19 of 21 segments\n",
      "Episode 118: Kept 8 of 12 segments\n",
      "Episode 119: Kept 19 of 20 segments\n",
      "Episode 120: Kept 11 of 12 segments\n",
      "Episode 121: Kept 17 of 18 segments\n",
      "Episode 122: Kept 31 of 33 segments\n",
      "Episode 123: Kept 13 of 16 segments\n",
      "Episode 124: Kept 21 of 24 segments\n",
      "Episode 125: Kept 11 of 12 segments\n",
      "Episode 126: Kept 13 of 15 segments\n",
      "Episode 127: Kept 23 of 24 segments\n",
      "Episode 128: Kept 8 of 12 segments\n",
      "Episode 129: Kept 22 of 22 segments\n",
      "Episode 130: Kept 11 of 12 segments\n",
      "Episode 131: Kept 18 of 20 segments\n",
      "Episode 132: Kept 13 of 13 segments\n",
      "Episode 133: Kept 15 of 17 segments\n",
      "Episode 134: Kept 10 of 24 segments\n",
      "Episode 135: Kept 19 of 19 segments\n",
      "Episode 136: Kept 19 of 19 segments\n",
      "Episode 137: Kept 17 of 17 segments\n",
      "Episode 138: Kept 24 of 27 segments\n",
      "Episode 139: Kept 9 of 9 segments\n",
      "Episode 140: Kept 10 of 17 segments\n",
      "Episode 141: Kept 12 of 13 segments\n",
      "Episode 142: Kept 26 of 30 segments\n",
      "Episode 143: Kept 28 of 29 segments\n",
      "Episode 144: Kept 14 of 15 segments\n",
      "Episode 145: Kept 17 of 17 segments\n",
      "Episode 146: Kept 10 of 16 segments\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# apply character filter to saved JSON files\n",
    "filter_segments_by_length(PODCAST_NAME, START_INDEX, END_INDEX, MIN_CHARACTERS, MAX_CHARACTERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
