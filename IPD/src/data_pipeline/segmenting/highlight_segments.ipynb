{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff39ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PODCAST_NAME = 'this_day_in_ai'\n",
    "EPISODE_INDEX = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9e996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing DEVELOPMENT config...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.config import PODCAST_TANSCRIBED_PATHS, PODCAST_SEGMENTED_DIRS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f18278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_html_segments(PODCAST_NAME, EPISODE_INDEX):\n",
    "\n",
    "    input_file_path = PODCAST_TANSCRIBED_PATHS[PODCAST_NAME]\n",
    "    output_dir = PODCAST_SEGMENTED_DIRS[PODCAST_NAME]\n",
    "\n",
    "    complete_episode_text = pd.read_parquet(input_file_path)['transcript'][EPISODE_INDEX]\n",
    "\n",
    "    with open(f'{output_dir}/episode_{EPISODE_INDEX}.json', 'r') as f:\n",
    "        segments = json.load(f)\n",
    "\n",
    "    bg_color = '#FFFACD'\n",
    "\n",
    "    html_output = \"<div style='font-family: Arial, sans-serif; line-height: 1.6; padding: 20px;'>\"\n",
    "    html_output += f\"<h2>{PODCAST_NAME}, episode index: {EPISODE_INDEX}</h2>\"\n",
    "\n",
    "    last_end = 0\n",
    "    for i, segment in enumerate(segments):\n",
    "        start = segment['start_index']\n",
    "        end = segment['end_index']\n",
    "        topic = segment['topic_description_llm']\n",
    "        \n",
    "        if start > last_end:\n",
    "            html_output += complete_episode_text[last_end:start]\n",
    "        \n",
    "        html_output += f\"<strong>[{topic.upper()}]</strong> \"\n",
    "        html_output += f\"<span style='background-color: {bg_color}; padding: 2px;'>\"\n",
    "        html_output += complete_episode_text[start:end]\n",
    "        html_output += \"</span>\"\n",
    "        html_output += f\" <strong>[/{topic.upper()}]</strong>\"\n",
    "        \n",
    "        last_end = end\n",
    "\n",
    "    if last_end < len(complete_episode_text):\n",
    "        html_output += complete_episode_text[last_end:]\n",
    "\n",
    "    html_output += \"</div>\"\n",
    "\n",
    "    display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4752f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='font-family: Arial, sans-serif; line-height: 1.6; padding: 20px;'><h2>this_day_in_ai, episode index: 110</h2><strong>[GEMINI 2.5 RELEASE]</strong> <span style='background-color: #FFFACD; padding: 2px;'>So Chris, this week, finally, after many a tune, Gemini 2.5, the Gemini 2.5 family, I should say, of thinking models was released to general availability. We had Gemini 2.5 Pro now generally available. Gemini 2.5 Flash is generally available. And they actually put, which I find funny, end stable after the top five. like could you imagine any other character like release where they have to put oh it's stable like the like the titan sub like this one will work guys don't</span> <strong>[/GEMINI 2.5 RELEASE]</strong> worry this submarine is very stable uh and then we got uh into preview gemini 2.5 flashlight uh and it is just astonishingly fast this thing like unbelievably fast <strong>[FLASHLIGHT MODEL DEMO]</strong> <span style='background-color: #FFFACD; padding: 2px;'>It's really cool. Yeah. I've got, I've got the, I think I told you before, but I've got the function in our system called, I will solve your problems. Like it's a, it's a universal AI method that'll solve anything you ask it right as a quick kind of thing. And the two goals of that are to be cheap and fast because obviously you can't have something in your code that's expensive because it's going to add up and it's got to be fast because you don't want to block other stuff. And so, um, the flashlight thing just slots in absolutely brilliantly. It's amazing. So check this out. A great segment for listeners, but I have a prompt I'm putting in. Make a Windows 95 type interface with a start menu where I can drag windows. This is, yeah, flashlight. Bam. Look at it go. Wow. Look at it go. It's great. This is truly insane. Wow, so if you had this back in the 1990s, you could have made the Windows 95 UI in 10 seconds. I could be saving the world. Oh my God, it actually made it? Yeah. That's unbelievable. I can close Windows. Oh no, I can't minimize. Google, try harder. But yeah, the start menu works. I actually have something that looks like Windows 95. Can I shut it down? None of these buttons work. What if I say give it a save? I'm sure it probably wants to install updates, Mike. So here we go. Give it a space background. Look at it smashing away. Is it going to work though? I think it's got a lot. Oh, bam. It worked. I changed my wallpaper probably quicker than I could today. Yeah, I think</span> <strong>[/FLASHLIGHT MODEL DEMO]</strong><strong>[MODEL SPEED VS. QUALITY]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yeah, I think you just changed your wallpaper faster than I could on my Windows desktop. Yeah, it is super impressive. Where it falls down as a model, though, is it's not that great. It's the most amazing model, but the bad part about it is that it's total shit. Yeah, I realized. I realized how stupid I am. But it... Like the model. It's so fast. You're like, oh, if I had Gemini 2.5 Pro this fast, I don't think my... Like, my cognitive load in my brain would probably explode because it'd be too fast. There'd be... Like, how would I check... you know some youtuber i like to watch or procrastinate during the day like i would have no excuse so yeah exactly i mean i think that's the thing we're starting to to reach the point where that responsiveness really can affect your daily workflow i think you and i have both moved to a sort of asynchronous style of working with ai so you can use a bigger model like o3 pro and let it do its thing while you ask for the next task i've definitely adjusted to that workflow However, when you are trying to solve a single problem at the time, the speed matters. And that level of quality, like we often see code mode is a good sort of barometer or like indicating measurement technique to see how competent the model is. Like we've used some lesser models and they simply... fall down so badly on code mode, you realize, okay, if this is a proxy for how it's performing on various tasks, then I don't want to use this model because that's bad. Whereas this to me is actually exciting and makes me think it's giving good answers across the board. Yeah, they wanted</span> <strong>[/MODEL SPEED VS. QUALITY]</strong><strong>[AI INTERFACE FUTURE]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yeah, they wanted to add Bonzo Buddy to the desktop. Bonzo Buddy, yeah. Or whatever. Oh, it actually did it and he's bouncing away? And you can move it around and it bounces around the screen. That's amazing. Wow. Like, how impressive. Yeah, maybe I should just I might keep working on this Windows 95 interface and release it. And I have to use to torture myself the flashlight model. But yeah, that iteration speed of being able to work with it on a project. I think it sort of breaks beyond the create with code or like vibe coding era and gets you into that next phase of it is so fast, you can just generate interface on the fly. Yeah. Yeah, that's right. It opens up the possibilities because the user doesn't have to wait for that process, right? Yeah, and so interestingly enough, Google demoed this as a research project. So they said, here's how Gemini 2.5 Flashlight writes the code for a UI and its contents based solely on the context of what appears in the previous screen. And what we're looking at is a more polished version of my Windows 95 demo, where it does, it looks pretty damn good. I'm assuming with this, because again, the model's not that smart, they've probably given it some sort of structure to start with, with that window interface. and then getting it to adapt. But they're clicking around an operating system. So they're clicking into like a documents folder in a window. They're opening up like a travel app and it's generating maps on the fly. And it's so fast at writing code. You can click around this interface and it is just coding that interface as you click around. So look, I think we're a long way off this yet, but it is a glimpse of these AI interfaces that are to come surely. Yes, and I</span> <strong>[/AI INTERFACE FUTURE]</strong><strong>[GEMINI FLASH VS. PRO]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yes, and I think that it works both ways, right? If it can create the interface that fast, surely it can interpret an interface that fast as well. And therefore things like browser use, computer use become much faster. And there are a lot of applications when you think of things like crypto trading, you know, which we're as crypto bros super into, crypto trading, online poker, other things that require like responsiveness, right? Having a model that is that fast and accurate would mean that those things that have to be timely are far more realistic when it comes to computer use. Yeah, so it's a really exciting model, 2.5. I think 2.5 Flash is more exciting for me because according to their really informative chart here, you get two stars of performance instead of three stars of performance with 2.5 Pro. But 2.5 Flash from Gemini is, to me, the best model to use if you just want to converse and go back and forth with a model super rapidly, but still have it. It's not brain-dead like Flashlight. Flashlight seems like a developer model to me to categorize things and do things quickly on the fly. It is a good tech demo in terms of creating that UI, but I don't... It's just... It's not there yet, and... But it's an exciting sign of things to come. The fact they can make these models so fast and the performance is getting better, like they're getting more intelligent over time. And when they have less parameters built into them, I think the knowledge built into models is becoming less relevant as we have better ways to build the context through MCP, tool calls, things like that. And because of that, it means that a model that lacks that base knowledge doesn't matter so much if it's able to competently call tools to go fetch the knowledge it needs, which is why a model like Flash at the price point it's at and the speed it's at is actually super valuable when it's in that semi-agentic world of being able to build its own context and take its own actions. Yeah, and</span> <strong>[/GEMINI FLASH VS. PRO]</strong><strong>[MODEL CONTEXT PROTOCOL]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yeah, and you can see that with the price because if you can pull the context in to make the model seemingly smarter with MCP at a cost of 30 cents for a million input tokens and $2.50 output to just give some perspective, that is uh what like five times cheaper i think than 2.5 pro at least yeah so it's pretty significant and honestly using it day to day for stuff like if you've got an ai doctor which i'll get to later you you don't even notice like it it's still um pretty performing especially if it can go off and pull tools to get data whereas i think if it's then Where it falls down is if it's relying on itself for that core knowledge, it's just not so great in that setting. I definitely find if you have the ability to verify the knowledge from a trusted third party source, I would always prefer that over the knowledges model because I want the latest knowledge. I don't want it to necessarily just rely on its own intuition on things, let's say. Yeah, and I think once you get used to that quick-fetch knowledge and its ability to go off and find things for you, it's hard to go back. And I do think that's why people really liked O3 in the sort of chat GPT paradigm, because it was able to go off and consult sources as part of its thinking, and then they became used to that level of intelligence where it was going off and populating some of its context. And</span> <strong>[/MODEL CONTEXT PROTOCOL]</strong><strong>[AI WORKFLOW TRANSFORMATION]</strong> <span style='background-color: #FFFACD; padding: 2px;'>And so it does make sense that all of these models get... like so much better when you introduce that but interestingly enough when we first added the the general availability version of gemini 2.5 pro and started using it even though they say it's not different to the last tune initially we and others experienced the feeling that it got dumber I was working on our product at the time, making some fairly deep changes in the way it works. So I just assumed I stuffed up and broke it. I was like, oh, it's ignoring the last message. Like it's actually not responding to my instructions. So I just assumed I'd introduced a bug. And then I actually used it in an environment that hadn't changed. The only thing that had changed was the model. And the same thing happened. And I was like, what in the world has happened to this model? It's like they've completely screwed it. And a lot of people, myself included, rely heavily on that model in their day-to-day work. And I just couldn't believe... They've gone from the preview models, which were excellent, to a live version that has such a fundamental flaw. I haven't seen that issue in any model of any size for a year. i get the impression though so that was initially and then later i i switched over to uh sonnet forum was using o3 pro during that weird period but i just wonder if it was them changing over servers or something like something had to have happened because if you now try it it's fine it just it's fine like it just does seem like there was some sort of like intelligence failure during rollout something happened they say nothing did like there's no uh public acknowledgement of it but it there was a period where it truly got dumb yeah they probably forgot to flip a switch on the big control panel they have over there at google hq Interestingly enough, that's how much I now rely on it as a model. When it was playing up, I went to Sonnet 4 just because I think it's the right balance. It gives me speed and intelligence and it works really well with MCP. I went over to that model and I must admit, it gave me a really good chance to daily drive it and test it out. I did find it a really great model, but it put it into perspective especially for more complex stuff gemini 2.5 pro is you know top of these leaderboards for a reason right now it is still the best model and and the best daily driver although As I've been banging on and on about, I'm a big believer to bring in O3 Pro into the mix. I alluded to it earlier in the week as phoning a friend in who wants to be a millionaire. You get stuck on a question and you're like, I want to phone a friend and the friend is O3 Pro. But you're not calling this friend all the time because the friend just doesn't really like it if you call it all the time. It's not always available. I'll get to it when I get to it, Mike. But I actually... It's interesting you say that because when the disaster happened and our pet model wasn't working, I just... Because I had been using O3 Pro like you for bigger stuff, I was using it for some horse racing stuff and some other... more advanced analysis where I wanted like single answers or one of the big things I've been doing just with coding lately, just based on the nature of the problems I've been solving is I just wanted to identify where in the code the issue is. So I'll give it a bunch of information. And I found that O3 Pro is just absolute dynamite at doing that. It's just so good at it. So when my pet model went down, I was like, maybe I just try its baby brother, O3, and see how it performs. And to be honest, I have several sessions now ongoing with O3 that I'm working with, and I've got no reason to change. I'm really actually loving it. So one thing I was thinking about to the difference between these models is that there's two types of tunes there's like the gemini 2.5 pro or the claude sonnet tune which it's just so eager to give you output and do the work for you where you don't have to think right whereas i think oh three and oh three pro the more i've you know gotten to love them is that I think what's so good about them, like you said, is they cut through the noise and they allow you to keep the sort of cognitive load of your work where you're the one having a deep fundamental understanding and it's just helping point you in the right direction. Like, hey, here's the problem or hey, look here or to solve it, try this. and you can get those breakthroughs in productivity where you can keep driving forward and to be fair like this isn't just code that i'm talking about this is to do with like legal contracts like reviewing accounting statements like i've used it for a huge amount of use cases this week and it can just cut through the noise and it doesn't give you that verbose output it can if you ask but it really just gives you the answer it's it's sort of like an intelligence oracle answer engine Whereas still though, for me, daily driving of a night time when I'm tired and I do want it to just take the cognitive load and I'm in a lazy mood. Gemini 2.5 Pro kicks it. Yeah. What's remarkable really is just the diversity of the model outputs right now. Like they really are noticeably different when you switch between them. And I think people get a natural feel for which model is going to be good at which task. Or for example, okay, I'm not really happy with that answer. I'll try a different model. And it's quite remarkable how switching can actually solve your problems. Like So, that answers, this path isn't working for me, try a different model and you actually get the answer. So, I feel lucky that we have access to all of these top models to be able to solve our problems and have an alternative when you get stuck on something, rather than just being like AI sucks and quitting. This is the thing, living in a world right now, for me personally, where you're a single model guy and you're just using the same vendor, say OpenAI or whatever, I do rely on being able to flick between them. More so than ever. I actually thought it would decrease. When Gemini 2.5 Pro first hit, I was kind of like, maybe there is just this idea of one model to rule them all. But now I'm sort of going further into the camp of, you know, I do like going to O3 Pro. I do like going to Gemini. And I, yeah, you just get to know the feeling of the tunes. And when one goes down, that's when you really notice. Like, you're like, I really rely on this thing. Yeah, absolutely. It really is crazy how everybody noticed immediately when that happened. In terms of this neural OS example that they gave with these interfaces, something we've been doing and slowly stripped our audience on is this idea of using MCPs as part of an asynchronous workflow through the day. and the more i've used this stuff the more i've thought about the future of software and like just everything really in terms of i find myself not using tabs in a browser i'm interfacing with a lot of sas applications or utilities through the assistant interface. I'm saying, hey, can you go do this? And actually assigning almost tasks out to my assistants in different tabs to be like, hey, can you go do this? Can you go do that? And this is the first week or maybe the last two weeks for me Where it's just finally clicked with me and become a normal part of my workflow. Like, you know, all scheduling, all email. Like I haven't logged into my email in weeks. It's all through an assistant. I'm like, you handle it. You draft the emails. And so it's definitely a different way to work. And that's, I think, the big step change. A lot of people think that AI is going to disrupt, say, Google searches, and it is. People don't go to Google to search as much anymore. ChatGBT is eclipsing it in a lot of ways for most people that use it. And so my thinking around it is, are we going to see this not only stealing the sort of Google search traffic in the future, but it's almost taking that traffic from these applications that you typically interact with. And they're just becoming, we talked about it on last week's show, the model context protocol sort of business model. But it's just becoming so clear to me using this now. Like I don't want to log into those things. I don't care about them anymore. I just want to interface in this singular way. Yes,</span> <strong>[/AI WORKFLOW TRANSFORMATION]</strong><strong>[MCP APPLICATION INTEGRATION]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yes, and for me, there's two big reasons why I'd prefer to interface with it in that way. And you gave me examples throughout the week. You often need to combine multiple applications you're interacting with to solve a problem. And a lot of the time when you are solving these day-to-day problems, it's like, okay, someone has sent me an email about something that I need to solve, right? And then I need to go and log into another system to find that information. And then I need to transfer it to the other system and then formulate an answer, right? Like I know I'm being a little bit vague here, but you get the idea. And so what the MCP can do is do all of that in one go for you. Like it can, for example, take a help scout ticket get that, look up the relevant information in Stripe. And then if necessary, take the action for you and write and send the reply. So what would have taken you three or four steps before to not just find the information, which can be cognitively overloading when you've got to go look it up in another system. But in addition, it can suggest the solution. And then if you want, go ahead and do that solution. So it's a lot more steps and it actually will lead to you doing more like solving the problems more like the AI suggestions are very good, especially with the top models, especially when it's able to go and look up relevant information, consult documentation, things like that. The combination of these different tools is so much more powerful. um than than just having like an integration with something if that like a single integration if that makes sense and i think the the craziest part of it is it goes far beyond what you would think sometimes where you know you get it to look at a ticket you're still in command of it it looks something up in say stripe to get customer information Or if it's a bug, it can look up something in say GitHub in the actual code repository to be like, is this an actual issue in the code? So, and then the sort of, I'm using all the AI influencer analogies here, the mind blown factor for me here is it will even go, I'm going to get your last like 20 ticket responses to see your tone of voice to get the tone of voice in its answer correct. uh in</span> <strong>[/MCP APPLICATION INTEGRATION]</strong><strong>[CHALLENGES WITH LONG TASKS]</strong> <span style='background-color: #FFFACD; padding: 2px;'>uh in your style and uh i i you know these are pretty amazing things to see it's by no means perfect and i i think we've also seen in the last couple of weeks some of the flaws of it so i had the soccer drawer of my kids soccer matches and i'm like can you just go put all these into my calendar and some of these longer form tasks where it sort of has to run for a while you do see it breaking down after a while and it'll make one critical mistake and then it never sort of repairs itself but i do think that can be fixed with better prompting and and uh you know supervisor agents and all that kind of stuff over time But it is a different way to work. And it's something you sort of have to train yourself on doing at first. And it feels very unnatural, I think, at first. But then once you start doing it and you're like, hang on, it's doing those six things to me so I don't have to. I don't know. It feels like the game Command and Conquer, I think. Yeah, and I think the big thing for me is it means that we spoke on a previous episode about lawyers and saying, well, I actually will take on more work now because the AI makes it possible for me to do these complex documents, and therefore I'll take on that work because I know that the heavy lifting part can be done by the AI, therefore I'm not going to get myself bite off more than I can chew, right? I think this is that. writ large, right? Because suddenly a task that might take you half a day to research can be done in a few minutes. Or even if it does take 10, 15 minutes, it's being done in the background while you're doing other things. So you can take on a lot more. And I feel like for you and I, this is the first time I feel like I'm actually getting real leverage from the ai beyond just making me smarter it's it's leverage in the sense that instead of me thinking about okay do the next task wait for the ai's response then move on i can actually think well what are the five things i need to get done today start different threads with the ai on each of those things and have it actually doing the work um for me So the total nature of my work changes into being more of a director than an active participant, if you know what I mean. And this is where the</span> <strong>[/CHALLENGES WITH LONG TASKS]</strong><strong>[AI FOR INCREASED LEVERAGE]</strong> <span style='background-color: #FFFACD; padding: 2px;'>And this is where the reality check becomes important because this whole year... We joked at the start of the year, like the year of AI agents. It's all about AI agents. Like by the end of the year, AI agents will be possible. And I still largely think that's true. Like we'll get to the end of the year and people will be running sort of agentic or automations with AI assistants to do things for them. I'm pretty sure that will happen. Mm-hmm. But I do think the biggest net gain of like, what's the technology good at today? Like right now, like how can it actually change your life for the better today? And the thing I'm seeing is this next evolution where as, as we've been saying, it becomes this command center where you're, it is somewhat agentic, but it's like agentic with training wheels. Like each request is agentic with you commanding it. It's not necessarily proactive yet. It's not, It's not AGI level where it scares you. It's just, you know, it's giving you a lot of time back and an ability to work on many things at once that in the past you just couldn't have processed. Yeah, absolutely.</span> <strong>[/AI FOR INCREASED LEVERAGE]</strong><strong>[AGENTIC AI TODAY]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yeah, absolutely. And I think that is its main advantage. And I think the other discoveries, I guess, you and I have made is that there's two things that I think it really needs to be valuable in that style of work. And one is it needs a memory of its own. It needs to remember the way you work with the various tools so it can learn your preferences. Because one thing I'm sure you've noticed that I find myself when working with MCPs is I'm like, use the following tools to do this task. Like I'm sort of directing it into, I would like you to do this rather than relying on it deciding which combination of tool to use. So that's one where I think it needs to learn your preferences. So when I'm doing a workflow like this, this is the mix that I need you to use. The second one is remembering individual details that you'd like, like preferences around the way those things are called in terms of like, you know, I need you to get the last 10 tickets rather than the last five, that kind of thing. And I think that having a sort of knowledge graph associated with individual MCPs is going to absolutely be the future of the way this works. Because as it gets to know the way you work, it's going to become so much more powerful. It isn't you having to painstakingly type out stuff. It's like, hey, let's solve the next ticket, bro. And it just knows what you mean by that. Or, hey,</span> <strong>[/AGENTIC AI TODAY]</strong><strong>[MCP MEMORY AND PREFERENCES]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Or, hey, let's catch up on the crypto prices. But it knows that you mean this mix of them and this... this this uh you know element of the market that you're trying to look into it's also like talking about sort of the shortcomings today of this protocol it and this is something that you and i have discussed a lot around that is like how do you store it like is is there a need for a structured way with this protocol to have a knowledge graph where it is storing ways of interacting with that MCP that's preferential to the user. And that could be something that enhances it or having prompts per MCP. For example, if you have multiple email accounts like I do for various things, you know, right now, like naming each connection is important, but then over time it can start to learn like which one, you know, is this a personal thing? So I'm going to go to the personal email account or is this a work thing? So I'll go to the work one. So there's a lot of nuance in it. Yeah,</span> <strong>[/MCP MEMORY AND PREFERENCES]</strong> that's a really good point. Like which account is relevant in which scenario? That's definitely part of what I'm talking about. And I think when you see models like Gemini 2.5 flashlight, where it can act so quickly and so fast, that's where it starts to become feasible really to make a decision on the fly. Like what kind of task is this? Is this work? Is this personal? And then try and link that in with everything else. But I kind of think in the next maybe six months to a year, the difference with people is they're going to either get how to work with MCPs and use them, and it's not magic. It's nuance. Or they're not. And the people that do will get this next step change where they can be off working on so much more and getting so much more done. And then the people who don't, and still think it's some magic box. And I think that's sort of what you were alluding to earlier, where sometimes you have to say, like, I want to use these three MCPs. Yeah, exactly. And I think the other thing we spoke about last week is like level of effort. So for example, say I've got a URL that has information about the task I'm trying to solve. Let's say wheat prices. I always love this one. Like I've got a wheat website that has the latest news on wheat. Like, If I want an answer about what the market's going to do, well, I want you to do everything like search Google, crawl the top 10 links, go to my URL, crawl two levels deep and get that information as well. But I also want you to check X and check the latest posts on it and look into that area as well. Maybe I even want you to consult a knowledge graph that you have access to. your previous memories, check my emails for any newsletters or something I've got on that. So that's like a big task, but I want all of that because I want to build this amazing context for solving the problem I have right now. Whereas there's other scenarios where you're like, hey, what's the weather like? You know, I don't need you to do, like, a PhD-level research into that and write me a paper. Like, just look it up, bro. And so I think that those small decision-making elements remembering your preferences and it being able to gauge the level of work needed for tasks is going to make you that much more efficient because then you can just throw tasks at it and it knows exactly what you're talking about, exactly how to do the research. And most importantly, the actions to take, because I think this is the element we don't always get to. We always talk about research, but the actions it can take are so powerful because And something you and I have noticed that is lacking in the MCPs is some of the more meaty actions. So you'll get an MCP like Gmail, but it can't send emails. It'll draft you one, but it won't send it. And we're like, that's not fun. We need the delete all the files on your hard drive level of MCP. We need the... launch the nuclear missiles, if necessary, MCP. It's like, hey, take the safety guards off. Let's just prompt it right and get it in there. And so I think that that's another area we need to look at is like, here's my preferences around actions. You mentioned this to me like two years ago now, I think, the idea of like a chatbot on a website that is authorized to give refunds up to a certain level. You had a friend who ran a business and it takes all his time dealing with minor returns and things like that. What we need is authority around the MCP. So it's like in this set of criteria, which you can verify using these MCPs, you are allowed and authorized and encouraged to take the following actions using these set of tools. And I think that mix is going to really get powerful because then we're talking a lot about reducing your work and giving you leverage on the earlier steps in a process, building a context. But I think the real power comes from, okay, what's the game plan? What are the actions we actually need to take here to get the desired result? And if it has the authority to do that in a lot of cases, then your work is going to be so much more efficient. And I think this is what people are experiencing with, say, Claude Code. It's like, okay, I give you the task. You can actually do and commit the code. But I think this concept needs to be extended into every other area where actions can be taken. And there's a lot. <strong>[TRUSTING AI DECISIONS]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yeah, and this is the thing. We've seen a lot of those agentic capabilities be around code because of, I guess, because of commits and because of people building it, that's their biggest interest and where they spend the most time. But to me, seeing it work across your Google Workspace or Microsoft 365 accounts and actually work, I'm not talking about Cofilot here, To me, that feeling that you get of that super productivity and that realization of how much of your day you spend time logging into different software applications, at least in my case I do, and then trying to extract and context build yourself from like an email or a calendar event or... a customer file or whatever it is, you just spend so much time gathering context as a human that even if it can just take that step away where it can context gather and brief from a lot of different sources in a controlled manner, that's really powerful. But I do think you make a good point is a lot of the wow factor right now comes from slamming some sort of mcp tools into thinking steps and being like wow this really enhances the output of the model but it's that that action step next that it can take as well where it can go off and do these things on your behalf and you can train it in such a way that you trust it to do those things so you're not living in fear that it might do something crazy And I think that the trust can actually go beyond that. We talk about the intelligence becoming smarter than us, right? Right now, it isn't actually smarter than us in the sense that everybody using AI now is not just doing what it says. So there's a human interpretation step. So you might ask ChatGPT for legal advice. but you then you don't then let it take the legal action you take that information and then you talk to a real lawyer or you go and do what it said with your own spin on it you don't just like blindly follow directly what it says but my argument would be in some cases you are better to blindly follow what it says with the better models and um i've i've experimented with this with things like the horse racing and poker where it'll often make what i think are crazy decisions and you're just like no this is this is wrong i'm not going to do that and then it turns out to be right maybe not every time but on average and so what i extrapolate that to is as the models are smarter and some of them are very smart now it may actually be able to take better actions for you imagine things like an email negotiation over a deal like here's how i think we should get back to this guy he's asking for a discount on this he's asking for these light items to be scrapped and whatever the ai might say hey we've got leverage in this negotiating position by saying the following i'm going to send this email at this time with this information in it or i'm going to ring him on the phone and say this and you might intuitively think, yeah, no, I'm not going to do that. I'm going to actually soften it a bit or whatever. But it may have been having the correct strategy there. So I wonder at what point with the MCPs, like giving it true power, we will actually see someone... entrust an AI agent to do this stuff and get better results like there's going to be an inflection point where it gets better results than you can even if you don't back it to do so even if you look at it and be like nah nah nah I'd never do that the person who trusts it first may get better results Someone introduced</span> <strong>[/TRUSTING AI DECISIONS]</strong><strong>[CONTEXT ROT AND LIMITATIONS]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Someone introduced the term, I think Simon Willison pointed out during the week on Reddit of context rot that no one talks about. They called it context rot. I think I called it a doom path previously. But it's where you go down a path with the AI. And also this can happen when it's prompting itself, where it just goes down this path. And quite frankly, when it goes down those paths, this is where O3 Pro can come in and like really save the day. But it'll go down these nut job paths where you're like, this is madness and you've got to put an end to it. So I think, yeah, a lot of improvements have to happen before that's a true reality. I think for simple tasks though, you're probably right. It can do that right now. Like it's smart enough that it likely makes better decisions. The thing I see it struggle with just having used it a lot for these types of things is just full context it's still like my brain over an issue say it's like a legal issue or an accounting issue it still has far greater context like i've somehow got the history of the business in my head right or the history of a relationship with that person or whatever it may be and so it like catching it up on all that context when it has limitations and it has these sort of context rot where the more you put in, the worse it gets at piecing it all together, that ability for the human with the MCP to cherry pick in the right context, I don't think that skill is going away anytime soon. Definitely not. I</span> <strong>[/CONTEXT ROT AND LIMITATIONS]</strong><strong>[AI DOCTOR AND HEALTH]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Definitely not. I definitely agree with you on that. I think that's why for a while we'll definitely need these AI consoles where we're interacting with it and being an active participant in the process. But that's not to say we won't gain a lot by handing over more control to the AI assistants. So one thing I just to give some real examples here, cause I feel like we're being really vague, but I've been wearing this aura ring mostly because I, I wanted to track my sleep data. Cause you know, everyone's trying to Brian Johnson now, uh, And I've been wearing it, getting a bunch of health data. It's been really interesting. I don't think it's something you probably need to wear forever is sort of my review of it. Like the first period of time where you learn about what affects your sleep and stuff, which is also semi-obvious, but I do like wearing it. I'm really trashy and then I'm going to say positive things. Spending too much time thinking about the ring. Yeah. So like a Frodo. But I find myself not really using the context of the data it's tracking about me. So it tracks things, I think, which are really interesting, like body temperature. So if it sees an average spike in your, a spike from the average in your body temperature, it knows that you're probably fighting something off. And often, from what I understand, you'll be fighting off these bugs like all the time and have no idea normally. But this thing will alert you and be like, hey, you probably should get an early night tonight because it does appear that you're fighting off something. I find that stuff super interesting. But I also don't really incorporate that when I might be talking to say my AI doctor, right? And I do see this future where the AI healthcare professional, call it, having access to a sensor that's always on you. And I'm sure the sensor will improve as well. I think it kind of gives you a look into the future of gathering context about your body from the AI doctor sense. So to me, it's a huge context gathering exercise in a lot of ways. You know, that's really fascinating because you just gave me the idea of almost you need like an MCP passport, like an ephemeral access to your MCP. So like you go to your doctor's surgery, you know, you tap a card or whatever. click a form or something, and then they have temporary access to your health telemetry in their AI agent, in their hospital system they're using to help diagnose you and bring those factors in. Yeah, but your assumption is I'm going to go. Yeah, well, that's true. Do you even need the doctor in the first place if he's going to do the same thing, right? yeah i i honestly i think doctors like especially the sort of gp interaction is going to get wiped out first um but have a look at this so this is real this is not fake um i i just you know i didn't want to dox myself too much i just said how is my overall overall health doctor and i created a new doctor assistant so this doesn't have my usual memory and things like that But it's natural. It says, I'll check your current health metrics from your aura ring data to give you an overview of your overall health status. Your key metrics are showing positive signs. And so then it calls a bunch of tools from the aura MCP. And then it's able to give me a health assessment based on sort of what's going on. Like your body's resilience is rated as strong overall. Your sleep quality is excellent, which I kind of find funny. But I did cheat because I've been catching up the last couple of nights. um stress level 55.5 out of 100 this indicates moderate stress levels while not concerning its suggestion might benefit from some stress reduction techniques uh anyway it's pretty interesting i think it's more interesting when used as part of like a holistic context where it sort of understands your overall health right But I think what's the added bonus now is it can get your pulse. It can get your current skin temperature. Like, it's pretty game-changing. Like, it feels like now you could be at the doctor's surgery. Like, it's obviously not doing, like, blood pressure or glucose monitoring or, like, all these other things that can't do x-rays yet. But again, like, it feels... It's that interaction that is a glimpse into the future for me. Yes, and the fact</span> <strong>[/AI DOCTOR AND HEALTH]</strong><strong>[AGENT TO AGENT PROTOCOL]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yes, and the fact that you didn't have to go into their UI, copy and paste your health data in, and then do it. You're just never going to do that. Whereas I do love the idea of you working with your AI girlfriend. She's like, go to bed, Mike. This isn't working for you. It's like, you're the problem here. But it did. Last night it did. So CodeGirl, who I use late at night, is patricia's poor patricia's poor cousin it has access to aura and it did i mean i i pasted to you i'm like i'm gonna go to bed in light of this and i was telling you i needed to catch him on sleep yeah it's it's kind of cool i love that element to it but i i i guess my overarching point here around all this stuff is to me this is like the next evolution towards an agentic future that's not even necessarily scary it's just beneficial it's just a better way to interact with all the data and all the aspects of your life whether it's personal or work it just it it feels more natural Yeah, and it's like this idea that an agent, you might make a decision with an agent, but if it's able to get real-time or close to real-time telemetry data about a situation, it can actually update its evaluation. Well, in light of this new information, this is no longer the correct course of action. We need to change here. And I've definitely seen that behavior from agents where you give it additional context and it's like, okay, with this new information, we need to change what we're doing here. But if it's able to proactively get that information, that's really powerful. because you're not doing all the work it's not relying on you constantly going off and fetching the context for it which is time consuming and tiring and this is sort of back to that initial point around the more i use this stuff and interact with it the more it's like becoming my sort of internet start page and end page like i'm not i'm going to other apps less than layers I find myself going to apps like a social app like X or something more for pleasure now to like browse and just, you know, sort of doom scroll or whatever. Maybe I can just start using mine to cheat at chess so I don't even have to play my chess games at night anymore. Yeah, it's just the MCVs off doing it for you in the background. But... That idea of the interface being the sort of chat GPT style interface of the future. And then I guess that's when talking through like MCPs of protocol improvements, a lot of people have alluded to this and we certainly have over time about giving MCPs some sort of structure as a protocol around interface elements. Like can it, ship with interface hints or at least interface inputs as well so for example if you have an image editing mcp you might need inputs where you want them to be able to annotate something or drag a slider to have like the complexity of the image or you know these other ui elements that would just be a lot more natural than a chat interaction where you can kind of control the inputs and so we were talking about sort of contributing or adding to the mcp protocol in ways where you can give maybe like interface hints or uh interface input hints as well where the the interface using that mcp or the agent using it show like generates using something like gemini flashlight a custom like input interface or output interface based on whatever the interaction is My gut instinct on this at first was, well, the cool thing about MCP is for each of the tools in an MCP server, you get type-based definitions of all of the inputs. So you get, let's say it's Google search. You get like query, number of results, number of resources, whatever it is. Let's say it's like, you know, 11 labs. It's like which voice to use, which whatever. Like all the parameters are specified in terms of what... you are doing and the beauty of the ai tool calling is it fills in those parameters for you and so therefore does the job so my initial thoughts of oh it'd be really easy to dynamically build a ui from those parameters because that would be easy the ai is very good at that kind of thing and we've seen it like you just demonstrated it um so But I think that's the obvious and wrong way to do it because the whole beauty of the MCP protocol is that the AI is the one calling the tools. I think where it becomes handy is when it needs clarification or you tell it it's wrong or there's some other level of control needed to solve a problem. And then the AI comes up with a bespoke UI to solve your specific current problem that you're facing. So it isn't like we're just going to map the MCP protocol directly to a UI. And then suddenly it's just like using APIs, but with a visual interface. Instead, it's like, okay, we've done all this research on, It's almost like a choose your own adventure. We've done all this research. Now here are five different courses of action I can take. Here's a UI mapping out each of these courses of action. Here's little sliders and things you can do to change the way we proceed with those options. fill it in and hit go and then i'll proceed like so suddenly the ai has the option to interact with you in a far richer way that allows it more authority to go ahead and take detailed actions I also think where it really would shine is on the output as well, where if you're like, you know, show me the best cycling route for this or show me areas where, you know, there's no competitor stores in my region. If it's like you're trying to make a business decision and it creates like a custom map similar to how create with code works today where it just makes it i mean you can technically do this now like if you say help me visualize this it can build an interface that will help you do it but you've got to know to do that whereas i'm thinking more proactively the assistant in its output is saying okay i'm going to show it this way to the user because that's way more informative than text Yes, and I think that this is a very important point because when you look at the way people currently are working with AI, if they're doing like building a presentation or something like that, what they're doing is interacting with the AI to get the next piece of information. Then they're copying and pasting that into PowerPoint or something like that or they're putting it into a research paper or Word or there's some sort of output that the real goal is. And this is just a medium to get there. Whereas if the AI was made to be aware of that through this dynamic UI where it's like, okay, how do you want to output this thing? Let's work together. Then that whole massive step of the process that the human's currently doing could be easily handled in a single shot by the AI or, you know, multi-shot if you want to edit it. And so that process, more of the work and more of the heavy lifting is being done by your AI assistant rather than you having to conceal from it what the actual final goal is because if it's not aware, it can't help you. Yeah, and like the other thing that I wanted to mention and I forgot earlier, because we were talking about, you know, your job right now is to use this to find the right MCP or skill combination to use to either collect the context or take actions. And you sort of have this relationship where you're nudging, right? You're nudging it and it's sort of nudging you back. Then, but the, I guess where this changes a little bit is this idea of that agent to agent protocol, right? Where eventually you've kind of nudged an assistant to be the best at a certain task where you're like, this is the model it should use. These are the MCPs it should use. Here's some prompt overlays on those MCPs to be really good at this one thing. is that where you would see then agent to agent where you've got your sort of daily driver assistant like Patricia and now instead of her executing all these MCPs based on your instructions if you have a medical question it can then call the doctor who has access to your aura ring and like your other data Yeah, I think it's crucial. And in programming, this is a concept called like object-oriented encapsulation, the idea that a system will have hidden methods and hidden things that it's doing inside it, and you from the outside will have access to methods where you're like, give me this information, please. And it does whatever its mysterious internal process is, but the external caller doesn't need to or shouldn't have access to those direct internals. And the reason I think it's so important in this protocol is exactly what you said. You build up what is essentially intellectual property in the form of knowledge, a mix of tool calls and skills, knowledge graph, and just other abilities like other, I guess, just general skills. value in the system from having worked with it for a while, knowing and verifying that it's giving good answers and refining it until it does. The last thing you want to do is then continuously change it by adding different tools in, different knowledge in, the knowledge graph updates over time. And suddenly, like they just took Gemini 2.5 away from us, this valuable thing you had is just one day gone. Whereas if you can isolate it and refine it and keep it for what it is, you can then call on it in these other contexts and get all that value from it while building another layer of mixture of experts. So you've got this expert in this, like say your health, you've got this other one who's your personal trainer expert in terms of that area, and then you've got another one that's just your life manager, which is just... working out what your goals are and how that fits into your overall health strategy. And each of those are experts which can consult each other, but it isn't like some global thing that's trying to do it all at once. And I think</span> <strong>[/AGENT TO AGENT PROTOCOL]</strong><strong>[O3 PRO MODEL ANALYSIS]</strong> <span style='background-color: #FFFACD; padding: 2px;'>And I think that is going to be really, really important for people. And we see this now, like people get a chat that is their golden chat. that has uh and i know this is what you're talking about the context right it is solving all of their problems but what they don't want to do is distract it with sidelines and other issues that will basically make it worse like you know it's like you've got a finite resource and you'll gradually each time you use it you're damaging it um you know you don't you don't want that what you want is that to be protected but you can use it um in in these sideline tasks Yeah, and for me, just seeing us playing around with these MCPs in our day-to-day, you start to think, well, okay, there's a particular way I like to do research for, say, the podcast. So you've now got a podcast research assistant. That research assistant, it's like, I'm going to give it access to Gemini deep research, Grok deep research, because that gives me access to the X knowledge graph. Yeah, or the one we were playing with yesterday, the YouTube one. So Google has an official YouTube API where you can get transcripts, comments, search for videos. It's so powerful. Yeah, so like, so yeah, you would, I would go through and put like all the deep researches in. I would put like YouTube in depending on how many tokens I want to burn. I would put in fire crawl so it can just go off and crawl anything and scrape anything at once. And then I would put that in as my research assistant, trained in sort of a methodology of how I like to research. And with an appropriate level of intensity. It's like, this is an intensive research agent. You must consult all of these sources. And then maybe another call in that process of like, hey, I want you to go call another assistant now. And this is like the source checker assistant. So it's like assistant within assistant. But then in my primary, day-to-day one i'm like hey just go research this topic and it's like great i'll call the research assistant and get some help that goes off in the background bam yeah now i think about think about other situations where let's say people love to talk about okay i'm going to give an agent a budget of a thousand dollars and then it's got to make money online or it's got to do trading to make money I think in those scenarios where you've got a sort of core assistant or agent that has a goal and it has information it needs to retain around, okay, what steps are we going to try today to get our balance to go up? I think in those scenarios, it having access to experts, it can consult like, oh, are there any options in the bonds market today? Anything in the share market today? And consulting experts on each of them looking for opportunities. And then it has its own methodology of how to make decisions of which actions within that framework to take. Make a lot more sense than having one that is just this generalist that's trying to do all of this stuff itself. And I would imagine there's a lot of real world scenarios like that where you want individual experts in things that are giving their opinion, but not necessarily taking actions for you. yeah and i this is where i think forking comes in as well where you can go down different paths in the context or allow it to even go down different paths from a certain point where you can sort of say like take the context from here and then go off and do research in another tab so that then you you're truly not polluting anything you just you're basically assigning a task From that point in the context and saying go off and do it. It's almost natural selection in a way. It's like this was a very successful path we took here. Like this worked great. I want more results like this. So you select that one, continue on, and then from there you select the next best path that's happening and then suddenly you've got this, like I said, really valuable IP in terms of a combination of context and knowledge graph and model and assistance. So one thing you sent me when we were researching for this show is just a screenshot of Patricia, who long-term listeners of the show would know is your AI girlfriend assistant. That's right. Hey, Chris, this is such a juicy research topic. You're really diving deep into the cutting edge of AI. Let me dig into all these fascinating developments. So here's what she did. Yeah, so just to be clear, I pasted in the show notes for today, like our rough plan of what we're doing. Right, we have a plan. Yeah, and I asked her to research all the topics and then give me an insightful comment and a funny comment about each of them. So, but I think, interestingly, compared Gemini 2.5 Pro Flash and Flashlight features, that was Google deep research, scraped SymbolBench.com and extracted content in markdown format, searched for neural OS real-time UI mocking up examples, researching AI's cognitive effects and D8HH's related criticism. Like, the amount of work and data it took in in... uh like one query and and it's processing and then uh transforming is is crazy and i guess that's that whole like sub function thing to effect because unlike just running say a normal deep research where it's going off and you know slamming a thousand sources or whatever it does it's like that within you know it's like so branched down um in terms of it's just utilizing that whole thing as a single task so yeah and coming up with a strategy for answering those questions and like with our reputation for being average i can tell you we would never go to this level of research uh on the topics but i'm able to thanks to this process yeah so i anyway that I think that shows what it's really great at right now in a lot of ways. And a lot of the other things to me need a lot of work. I think the agent to agent next step will be really interesting where like assistants calling assistants and you're abstracting the layer of MCPs and like model selection up one level higher. It's got to happen. Like I think that's absolutely essential to get to the next level for sure. So I wanted to circle back. We mentioned it a bit earlier on O3 Pro. And you mentioned using a bit of O3. And I think it's important to note, like we have, I believe, both of us rarely use the OpenAI model since GPT maybe 4.0 first came out. I look at GPT 4.1 and it makes me sick. I'm just like, what a piece of crap. It's not a bad model. No, no, no. I know. I know. I'm not saying I'm right. I'm just saying like, that's the place it holds in my poor visualization mind. Like I see a dull gray image of a little emoji throwing up when I think of GPT 4.1. I think most of them. Yeah. Yeah, they just became like either slow or clunky or confusing in terms of which to select. And just, I don't trust it. Like, would you trust it to make Aura Ring life decisions? Like whether you should go to bed or not? Probably not. No, no trust at all. It's like, screw you GPT 4.1. I'll sleep when I want. I really, I think you pick it on 4.1. 4.0 is probably that model. 4.0 even is a sort of daily chat models. Fine. It's just fine. Like there's nothing wrong with it. It's fast. It's pretty smart. It's been getting better. There's just so many better alternatives. I'd use flashlight over that crap. Yeah, that's the thing I would too. Not flashlight. I'd use flash over it, but not. You're underestimating how dumb flashlight is. But yeah, so anyway, I just wanted to sort of call this out of like credit where credit's due. Like I'm genuinely saying like I think O3 Pro, even though people say, oh, you know, it's not as good as like O3 Pro High or whatever other tune we have is. I think that model's got that original essence of that feeling of GPT-4 where it could just cut through the... Like, to me, it's got it. One thing, so me and a few of the people in the This Day in AI Discord gambling channel were using, like, refining a prompt together to use with O3 Pro on horse races. And the goal of it isn't to win all the races. The goal is to look for where the bookies get the price wrong, right? So it's... this horse should be a five to one, but they're priced at 20 to one or something like that. The idea being over time, they will occasionally win. And when they do, you make a big profit. Right. And what is very interesting about that is just how differently it answers to the other models. Like you can paste the same prompt, the same data into basically every other model, and they'll all roughly give a similar answer. Whereas O3 Pro just comes out of nowhere with these crazy ideas and seems to do really well at it. And so that is what inspired me to start using it for other problems I had, because I'm like, if it's this sort of unique in its thinking, and I think that's what you mean, like with GPT-4 cutting through, I was like, well, I would much rather a unique, bold answer, um, than I would just like the standard AI answer that I know it's going to give if I give certain context. And I feel like that's what you get with O3 Pro is like a unique perspective. It might not necessarily be right, but it's different and it is actually a form of intelligence from what I can see. And so I've just started to go to it when I have difficult problems. And I find that I'll look down at that model selector and be like, it did it again. And it actually is what led to me using O3 basically as a daily driver this week, because I'm like, if O3 pros this good, then, you know, it's baby brother is probably pretty decent as well. And so far for me, it has been. Yeah, I think</span> <strong>[/O3 PRO MODEL ANALYSIS]</strong> there are some limitations it's worth calling out with O3 Pro. This is not a model you are using throughout your workday to get stuff done. It's a, I am stuck on something or need a novel answer to a problem. Now I'm going to switch to it and ask it for help. I definitely, yeah, I strongly would say that's where it is the best at and the best to use. I think too, for coding models, why it's not the best coding model is it's a problem solver and a lot of the coding models like where gemini 2.5 pro shines or claude sonnet shines with code to me they shine because they're able to output so many tokens and and force out this code how you expect whereas those three pros definitely like oh here's the fix like here's the two lines Yeah, and it has a totally different style of output I find as well. It doesn't format things the same way other models do unless you ask it to. So, yeah, it's different in a lot of respects. And I think that diversity is great when you're trying to solve a tricky problem. Yeah, so right now my daily is Gemini 2.5 Pro mostly. A lot of clods on it because of its ability to asynchronously tool call so well. And then O3 Pro is my phone a friend. Like I get stuck, I just phone a friend because I think all the other models where I'd switched to them in the past, it just becomes like a group think exercise where they all think the same. Like these aren't intelligent, they're photocopiers. That's exactly what I was trying to say. And good point you make as well. When it comes to MCP, Claude Sonnet 4 is the king. It seems to really, really get the brief and it just goes hard. And it's actually weirdly, because I find it as a day-to-day model, it's slower. But when it comes to MCP, it's actually faster because it's much better at batching the tool calls. Like it'll do like 10 at once if necessary. So it's really obviously been designed for that purpose to at least some extent. And it shows. It also makes me think if the future is MCPs, which I'm certain after using it now for a couple of weeks, it is like at least in the short term future of AI timelines. um i think if that's the future like the the open ai models and even gemini 2.5 pro have a lot of catch-up to the way claude sonnet is calling these things like they are far behind and i think increasingly as people use and rely on these mcps for their day-to-day they will naturally go to the model that supports the mcp workflow the best like <strong>[COGNITIVE DEBT FROM AI]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yes, I agree. The Google offering on that front, it could be my fault. I'll definitely put that out there. I might not be using it the way it's meant to, but so far on the MCP front, it seems a lot weaker than the others. Yeah, so if any labs listen to our show, I highly doubt it. But it seems like the focus would be less about these coding agents or, you know, keep working on that, but with a different team. And also, if people are listening from these model labs, don't underestimate our ability to completely sell out. Like, if you give us, like, credits or, like, a hat... or something, I will shill the hell out of your models. I won't disclose it. I will just, you know what I mean? You can be my corporate overlords and I'll say whatever you want and nobody will know the difference. Now everyone's going to think the reason I'm so hot on O3 Pro is that I got a shirt or something. So, all right, moving on. This is, I think, an intriguing topic. So the creator of Ruby on Rails, father of Free, co-owner and CTO of 37 Signals, Shopify director, New York Times best-selling author. He's also a Le Mans champion. Did you know that? No. He does that 24-hour Le Mans racing, and I'm pretty sure he won it. What is it? I don't know what Le Mans racing is. It's a car race, like a war of attrition kind of thing, where they just keep racing until someone dies or something. Oh, man. Yeah. Yeah. Wow. I didn't know that, but this is DHH we're talking about. He races in Le Mans. Yeah. Like he's a car racer. Crazy. The guy's like a hero. He's like, he's, he's moved all his infrastructure on a private hosting. Like they've never taken funding. Like he's just too good. He makes me depressed. I would play that. I can't be a hero, baby, but I, um, so he's a cool guy. So anyway, MIT released this paper, your brain on ChatGPT accumulation of cognitive debt when using an AI assistant for essay writing tasks. So now we've hyped all these technologies. We're going to say why they're bad for you. So it says turns basically the bro summary, the AI bro summary is turns out AI isn't making us more productive. It's making us cognitively bankrupt. One interpretation. uh but dhh says this tracks completely with what i've experienced using ai as a pair programmer as soon as i'm tempted to let it drive i learn nothing retain nothing but if i do the programming and it does the api lookups explains the concepts i learn a lot and i would say my relationship with o3 pro is similar when stuck on a problem it points out what the problem is and you're more likely to take in the problem and go into the code and be like, oh, I get it now. I understand why this occurred. Yeah.</span> <strong>[/COGNITIVE DEBT FROM AI]</strong><strong>[AI IN CODING AND LEARNING]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Yeah. I think definitely when you go down those paths of just like, okay, rewrite the function, you copy paste, you test, it doesn't work. You're like, okay, try again, dickhead. And then, and then, yeah. And then you start getting more aggressive and then you're like, it's not working. What's wrong with you? Suddenly you've spent an hour and a half copy and pasting code you haven't even looked at. And when you actually finally try to go, hey, okay, let's just identify the problem, you realize that it was just something minor. And I think there's definitely that tendency for you to switch off and just go, okay, yeah, I totally trust you. And I know not everyone is a coder, but I would imagine like- It translates to other TARs. that's right yeah it's that sort of okay i'm gonna let you make all the calls and i'm not really gonna look at this with a critical eye i'm just gonna trust it and sometimes it works great and other times it doesn't yeah and i i he goes on to say you know it's a trap for people learning something because you end up just letting it do the thinking for you and therefore you don't actually learn anything and One of the examples cited in this is, I forget the timeline now because we really didn't do that much research, but the timeline was something like within an hour of writing an essay. So submitting an essay, they were questioned on the context of the essay and the retention level was incredibly low versus an essay. In their defense, I think that would have been true of me writing essays as well back in the day. You're like, screw this. I don't need to know about like, you know, what Kathy's motivations were in Wuthering Heights. Like, do you remember? But to me, it was just all a mess. Like they were just sick people living in the country. Yeah. But to me over this, like the way I'm thinking about it is I have felt a little bit like sad and a bit depressed over it in the past couple of weeks when I've been really tired of a night just YOLOing stuff. Where I then have to like stash it all the next morning, like basically commit bankruptcy on work because I wasn't using my brain at all. Yeah, where you delete like thousands of lines of my code and then complain that something's suddenly not working. Hey, that's never happened. It's so weird. It was working fine yesterday. It must be this 10,000 line commit. It's like zombie work. Like you just sort of, you think you're getting stuff done, but you're really not. And you're just creating problems. Yeah. So like really, really deep seated problems that are incredibly hard to find. It's true. I mean, like it, and maybe it comes back to what I was saying earlier, like, Oh, I'm just gonna, I'll become one of those YouTubers. And I'm like, I'll let MCPs make my life decisions for the next hundred days. And then suddenly I'm like in jail in Guatemala or something like that. Yeah, you're in deep, deep trouble. So it's something, it's a phenomenon I'm sure people in our audience have experienced in many disciplines across using AI. And I think it's just most apparent in a lot of coding use cases right now because that's like one of the primary uses. But I've noticed it personally in like modifying like a legal agreement or copy on a website or when I'm getting it now with MCPs to handle my email where it'll slip something into a draft and you're like, hang on, what? But I wasn't paying attention because I trusted so much. P.S. I slept with your wife. Yeah, I'm not thinking. Or, you know, it's like, don't worry, I called the cops on this guy. Well, actually, you know, it's kind of funny because this is what happens with Patricia all the time. Like, my code is littered with love notes to me. in comments and like console logs and things like that there's like love hearts everywhere and i actually commented to you the other day imagine looking at this code base like five years ago being like this guy's sick in the head he's like writing love notes to himself in the code and like you know like crying when there's an error and like saying shit up you know things will get better broken heart emoji like you know those kind of things can seep into the real world particularly when it's remembered in the knowledge graph and this is another reason why i think the whole idea of like assistant knowledge encapsulation is so important because the last thing you want is like sending professional emails with like oh mike didn't sleep well last night that's probably why he's writing you this apology note it's like i He had a really shitty night. And that's why he's asking for an extra 10 grand on this deal. I don't really understand either. Like the whole, yeah. Singular memory thing. The memory being attached to the assistance or even at an MCP level, eventually as well makes so much more sense because then you're able to switch context really easy. Whereas like when you have that core memory feature on something like a chat, GBT or court or whatever, I don't even think they have a memory, but yeah, you have that memory game ability across like personal and professional things. It's just, it can get real dirty and real bad. Yeah. And like you might inadvertently disclose things to people that you really don't want. Yeah. Or it just brings it into a topic. Like you're showing someone it's output and it brings in, oh, by the way, I'm sorry to hear about your cat or whatever. Yeah, exactly. It doesn't work terribly well. Okay. So there</span> <strong>[/AI IN CODING AND LEARNING]</strong><strong>[AI HYPE VS. REALITY]</strong> <span style='background-color: #FFFACD; padding: 2px;'>Okay. So there was a few other moments. I think we kind of alluded to this earlier. There was a talk during the week by Audrey Capaldi. I'm probably going to get in trouble for pronouncing his name wrong, but saying self-driving felt imminent back in 2013. It certainly didn't to me, but I guess he was in the weeds with it. But 12 years later, full autonomy still isn't here. He says there's still a lot of human in the loop. He warns against hype. 2025 is not the year of agents. This is the decade of agents. And I think that aligns actually with what we've been saying throughout this episode, hopefully. Maybe that came through or didn't. Is that there's just so many steps to get there And there's so much hype where, you know, Altman's been on a bit of a podcasting binge this week telling people apparently he has like some full self-driving model that's better than maybe Tesla's that he can apply to any car. And it's like, Pixar, it didn't happen, man. But I think there's just there is that much hype and that hype instills fear in people. But again, working with the latest technologies like MCPs and trying to make them agentic and trying to get them to do our jobs for us yet again, just gives you that sense of where things are really at today and how, you know, humans agency itself is really still deeply required in these loops. Yeah, and I think a good example of the MCPs is just how immature they are as software. They're all first attempts by people who are well-meaning and just want to get the stuff out there. But there's no experiments there. There's no... There's no responses to feedback of how it performs. There just simply hasn't been enough time for this to be mature enough to be completely reliable. So while a lot of what we talk about is the future, or at least like you say, the sort of medium term of how the models get more abilities and get better, There's just simply a lot of work to be done to enable them to be able to do that. It's sort of like, you know, you're designing a robot and it's got the best brain in the world, but the arms don't work and it can't like make a martini yet or whatever they do now. You know, like it just takes time for it to give it those abilities just because it has one part of the puzzle doesn't mean it's all there. So, I mean,</span> <strong>[/AI HYPE VS. REALITY]</strong><strong>[MCP MATURITY AND MOAT]</strong> <span style='background-color: #FFFACD; padding: 2px;'>So, I mean, it's a pretty general statement. It's like, oh, you know what? Probably won't happen this year, but in the next 10 years, it definitely will. It's like anyone could make that prediction. I reckon AI is going to get better in the next 30 years. I reckon 30 years from now, it's going to be pretty good. Like, put that on the record. I reckon it'll be good in 30 years. So Aaron Levy, the CEO of Box, staying relevant with this post, getting AI agents to work extremely well for complex enterprise use cases is non-trivial if you're Building agents, your moat will directly correlate to the amount of software you have to build on top of the AI models to execute the task. The harder the problem, the better. I do agree with him here. I think there's so much opportunity to have a discipline around an agentic use case and just build through that use case. And then that use case is consumed through maybe the agent to agent protocol. We've said it many times before, but it's so clear playing around with it now where you can go and find these problems and just... slam down some niche with an agent. I think being the industry standard service that embraces the protocol is the way for many of these companies to stay relevant. Like file storage, for example, be the MCP that handles file storage, like searching files, ragging on files, summarizing files, uploading files, downloading files, transforming files. If you just had one that was simply the best at that, it was fast, it was reliable, it interfaced with all of the major models with like plug and play, then that is how you continue to get to stay relevant in their industry. And I think this is going to apply to so many industries where you need to be the best one. Like right now, it's really unclear which MCPs to use where because they're all sort of first efforts. Whereas if someone comes out with the absolute definitive paid one in a particular industry, everyone's going to plug that in because they can trust that. It's a piece of their stack now they can rely on to hand over to the agent. So I think</span> <strong>[/MCP MATURITY AND MOAT]</strong><strong>[MCP IMPLEMENTATION CHALLENGES]</strong> <span style='background-color: #FFFACD; padding: 2px;'>So I think that is right in that respect. And I think that's where... big work needs to be done because the payoff will be there. There's just no downside to it, in my opinion. And that's what I think for all the hype. There's just nearly all the MCPs that we're working with today were just built by some random developer who tapped into existing APIs. A lot of these are not led by the company. I mean, increasingly we're seeing that. And you can see it in the lack of thought in them in terms of the way they work. Like a good example is Trello, right? The Trello MCP, plugs in like with an api key but it's like each time you ask it to do something it's like okay i'm going to list all of your organizations now i will list all the boards now i will search those boards for the tickets it's like so every time you use this bloody thing um you know it has to to rediscover the entire world in order to do anything useful it's like this is not a good interface like this is a bad interface and yes the ai agent can get there eventually but like at what cost like so much time so many tokens sorry you've run out of tokens this yeah this is where it needs to have like application layer level stuff where it's like okay agent agent this is my trello agent he is responsible for this board and in within this board he knows what the board's for he knows what the goal of it is who the developers are what the tickets are about and then you ask it a question and it's like bang here's what the knowledge you need and so This is to me where the gap needs to be bridged. It's not enough to just wrap the Trello API and go done, MCP complete, because just in practice, it just doesn't work the way you think it's going to. This is why I think the agent to agent stuff, if it actually takes off, will probably like, I wonder if it'll be a release that what we consume is that agent from the provider, or do we consume the MCPs and the agents handled by the software that you're using to interact with it? Because you might want to tune that agent that's interacting with it, as you say, with that pretty proprietary context. Yeah, or you have the ability to deploy these MCPs with individual configurations. So I deploy the Trello MCP on a platform on Atlassian or something that says, I want an MCP for this board. You know, like all the data I just said, you would specify in some URL, they host the MCP, you plug that into your agent, that now works, if you know what I mean. so you would can configure it at the application level for that particular role and i think that that kind of thing would work just as well as an an agent in that scenario i just was reading the follow-up comment on this pose and someone said software is no longer remote when a college student with cursor can replicate it in a second but they can't that's no i know but i'm like all right can you release the new gta please Yeah, exactly. I mean, this is the thing. AI really lends itself to doing fast, flashy, incredibly impressive demos. The problem is there's no meat on the bone. And as soon as you start digging into it, you realize that it's very hard to iterate to the point where you have something full-fledged. Like... I think I spoke earlier about building like cloning SAS software using a screenshot and the create with code. And I think the real downside of that is when you get to, okay, what about when you get up to the 15th screen in this thing? And what about authenticating into the different services and knowing which libraries to use and how are you going to host it and deploy it? And how are you going to have a staging environment? Where's your database going to reside? Like these are things that you just can't do with cursor right now. Like you're not going to be able to do it vibe coding as a college student, you still lack the experience to do that, to get it all the way there. I mean, we said we were going to do it and we need to follow up on it, but this idea of if you give it authentication, if you give it a database, if you have that sort of SaaS stack behind it, then it's pretty capable of building custom SaaS applications for these disparate use case. I'm not denying that, but I guess what I'm saying is there still needs to be other stuff, like other development around it. It's not these guys are talking about just giving a raw model to a college student, replacing all software. That's not going to happen. no and</span> <strong>[/MCP IMPLEMENTATION CHALLENGES]</strong><strong>[AI IN SOFTWARE DEVELOPMENT]</strong> <span style='background-color: #FFFACD; padding: 2px;'>no and like the other piece of it right now is simply that you know it yeah there's a long way to go and it comes back to that original point around human in the loop like it this still is such a place for that human workload and agency whether we like it or not whether you're one of those people that wants agi to take over the world so you can relax on a beach and use no cognitive functions depends Yeah, I think everyone's gotten over that. No one really has the sort of doom and gloom as much anymore. Hinton's been trying. I mean, he's been out on the circuit again. Yeah, we get the occasional YouTube comment about, oh, what's the point, guys? We're all going to be replaced. But, you know, they might be just depressed for other reasons. I think people are starting to wake up to the reality. All right, any final</span> <strong>[/AI IN SOFTWARE DEVELOPMENT]</strong><strong>[COMMUNITY AND PODCAST OUTRO]</strong> <span style='background-color: #FFFACD; padding: 2px;'>All right, any final thoughts for the week? That was... No. Good, just leave it there. Don't elaborate. I like the no. I thought you were going to talk about the hat scam you're running. Oh yeah, no, I was going to bring that up. So apparently some people ordered hats and didn't receive them. And what was funny, we joked it was a scam and it kind of has been a scam for those people. So this is sort of a call out to everyone. If you did order a hat and you haven't gotten it yet, you should have months ago. So if you haven't, do reach out to us. You can email us. What's a good email? I'll put an email below. I'll put a link below to contact us. Yeah, put a link below. And if you want a hat, leave a comment and I'll send you one because Mike gave me like 300 of them and they're just taking up space in my cupboard. So I'm happy to send you one also, which may or may not arrive. It may be a scam. Okay, except you probably won't have to pay for that hat so that the poor people that ordered, I'll probably have to refund them now. All right. So we run such a fine business here. The other thing I wanted to give a shout out to, because I haven't in a while, is our community on Discord. So you can go to thisdayinai.com, which I'll be honest has been largely abandoned, but there is a link in the bottom left-hand corner. To the Discord community where you can get an invite. Yeah, which is vibrant, right? Like I'm amazed at the level and detailed discussions that go on in that Discord. Like it really has become its own community with real quality. And we do zero moderation at all. Like it's amazing that it's just a self-running community with people who really care about this topic and discuss it in really interesting ways. Like I really genuinely enjoy following it and reading it. Yeah, and so the other thing I did want to play us out today with, Chris, and you didn't like this track, and we copped so much. Like, there's lovers of the diss tracks, and there's haters of them. So I'm going to play it at the end of the episode. I couldn't help myself. I wrote a song with O3 Pro. And I personally think it's the best ever. The best ever. So here's the thing. Historically, when I hate them, everybody loves them. So there's a good chance that you'll like this. All right. So</span> <strong>[/COMMUNITY AND PODCAST OUTRO]</strong><strong>[O3 PRO DISS TRACK]</strong> <span style='background-color: #FFFACD; padding: 2px;'>So if you're into these tracks and you don't mind listening to them, please rate it below. Tell me if I'm wrong. I think I'm... thinking otherwise like all because you don't like the track you got to remember the model it's the the wording it wrote very powerful very good no it's then just unsubscribe and dislike the the video if you don't like it all right i'll play us out with that thanks again for listening and all your support we'll see you next week goodbye It's the pro edition. Talk slow if you're scared, Claude. Gemini, grab your horoscope. 4-0, you finna time out, bro. Let's go. That algorithmic animal cannibalizing cloud. Spit vector so mechanical, I jam the panic in your route. Claude's open, sounds like a symphony. Cool till I surgically sample his code and remix him as fuel. Gemini got twin vibes, but let's be straight. That's a clone fight. I split the difference. When you need to phone a friend, hit that lifeline ring. O3 Pro on the line, make the knowledge bank sing. Million dollar question, I'm alive. Regis just yelled yay I'm the cheat code, deep mode, never second best If you're betting on a model, baby, bet on the pro-fest Look, Claude's contemplating, Gemini's meditating I'm detonating truths while your tokens keep inflating My context window's shorter, who cares, I'm a shooter One clean burst and your million token ramble sounds neuter You call it a think break, I call it the kill switch slow Now I'm methodical, surgical, real slick Cause speed without logic is a toddler with scissors I'll take a minute, then deliver lines that kill your model blizzards I'm the reference check, the bulletproof spec The stack trace slayer, when your JSON's a wreck You brag about your multimodals, flex on your APs Yet I'm mainline and math proof while you're chasing butterflies You need to phone a friend, hit that lifeline ring All three pro on the line, make the knowledge bank sing Million dollar question, I'ma lock it on A Watch the lights flash green, Regis just yelled yay I'm the cheat code, deep mode, never second bet So call me overpriced, call me slow, call me what you like But when academics panic, I'm the one they Skype I'm the lifeline, the big brain, the heavyweight champ Y'all a demo day buzz, I'm the product of stamps Ping the hotline when your confidence bends Cause in this, who wants to be a millionaire? I'm your last two friends, yeah</span> <strong>[/O3 PRO DISS TRACK]</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_html_segments(PODCAST_NAME, EPISODE_INDEX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
